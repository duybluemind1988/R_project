---
title: "Job_attrition"
output: html_document
---
# 1. Import Data
```{r}
#library(tidyverse)# include dplyr, tidr, ggplot2, tibble, readr, purr
library(dplyr) # mutate, select, filter, summarize, arrange, group by 
library(tidyr) # gather, spread, separate, extract, unite, %>%
library(ggplot2)
library(tibble) # provide tibble class (better than traditional data frame)
library(readr) # read_csv, tsv, delim, table, log....
library(purrr) # map, map_dbl, split,
library(data.table)
library(rsample)   # for data splitting
library(caret)     # for model packages
```

```{r}
path="WA_Fn-UseC_-HR-Employee-Attrition.csv"
data <- fread(path)
data
```
# 2. Summary of data:

Questions we could Ask Ourselves:
- Columns and Observations: How many columns and observations is there in our dataset?
- Missing data: Are there any missing data in our dataset?
- Data Type: The different datatypes we are dealing in this dataset.
- Distribution of our Data: Is it right-skewed, left-skewed or symmetric? This might be useful especially if we are implementing any type of statistical analysis or even for modelling.
- Structure of our Data: Some datasets are a bit complex to work with however, the tidyverse package is really useful to deal with complex datasets.
- Meaning of our Data: What does our data mean? Most features in this dataset are ordinal variables which are similar to categorical variables however, ordering of those variables matter. A lot of the variables in this dataset have a range from 1-4 or 1-5, The lower the ordinal variable, the worse it is in this case. For instance, Job Satisfaction 1 = "Low" while 4 = "Very High".
- Label: What is our label in the dataset or in otherwords the output?
```{r}
#class(data)
#dim(data)
#str(data)
```

```{r}
# Using an insightful summary with skim and kable
data %>% glimpse()
```

```{r}
summary(data)
```

```{r}
psych::describe(data)
```
# Check Null value

```{r}
sum(is.na(data))
```


```{r}
#install.packages("Tmisc")
library(Tmisc)
Tmisc::gg_na(data)
```


```{r}
Tmisc::propmiss(data)
```
# Distribution of our Labels:
```{r}
#install.packages("cowplot")
library(cowplot)
```

```{r}
options(repr.plot.width=8, repr.plot.height=4)

# plot count
attritions_number <- data %>% 
                    group_by(Attrition) %>% 
                    summarise(Count=n()) %>%
ggplot(aes(x=Attrition, y=Count)) + geom_bar(stat="identity", fill="orange", color="grey40") + theme_bw() + coord_flip() + 
geom_text(aes(x=Attrition, y=0.01, label= Count),
            hjust=-0.8, vjust=-1, size=3, 
            colour="black", fontface="bold",
         angle=360) + labs(title="Employee Attrition (Amount)", x="Employee Attrition",y="Amount") + theme(plot.title=element_text(hjust=0.5))

# plot percentage
attrition_percentage <- data %>% group_by(Attrition) %>%
                                  summarise(Count=n()) %>% 
                                  mutate(pct=round(prop.table(Count),2) * 100) %>% 
ggplot(aes(x=Attrition, y=pct)) + geom_bar(stat="identity", fill = "dodgerblue", color="grey40") + 
geom_text(aes(x=Attrition, y=0.01, label= sprintf("%.2f%%", pct)),
            hjust=0.5, vjust=-3, size=4, 
            colour="black", fontface="bold") + theme_bw() + labs(x="Employee Attrition", y="Percentage") + 
labs(title="Employee Attrition (%)") + theme(plot.title=element_text(hjust=0.5))

cowplot::plot_grid(attritions_number, attrition_percentage, align="h", ncol=2)
```

```{r}
count(data,Attrition)
```
# 3. Data analysis/visualization

```{r}
ggplot(data,aes(x = Age,fill = Attrition)) +
  geom_density(alpha = 0.4) 
```

```{r}
head(data)
```

```{r}
ggplot(data,aes(x = Attrition,y = Age,fill=Attrition)) +
  geom_boxplot(alpha = 0.4) 
```


```{r}
ggplot(data, aes(x = BusinessTravel)) + 
  geom_bar()
```
```{r}
count(data,BusinessTravel)
```
```{r}
#install.packages("memisc")
library(memisc)
memisc::percent(data$BusinessTravel)
```



```{r}
ggplot(data, 
       aes(x = BusinessTravel, 
           y = ..count.. / sum(..count..))) + 
  geom_bar()
```


```{r}
plotdata <- data %>%
  count(BusinessTravel) %>%
  mutate(pct = n / sum(n),
         pctlabel = paste0(round(pct*100), "%"))
plotdata
```


```{r}
library(scales)
ggplot(plotdata, 
       aes(x = reorder(BusinessTravel, -pct),
           y = pct)) + 
  geom_bar(stat = "identity", 
           #fill = "indianred3", 
           color = "black") +
  geom_text(aes(label = pctlabel), 
            vjust = -0.25) +
  scale_y_continuous(labels = percent) +
  labs(x = "Race", 
       y = "Percent", 
       title  = "Participants by race")
```

```{r}
# create a summary dataset
library(dplyr)
plotdata <- data %>%
  group_by(Attrition, BusinessTravel) %>%
  summarize(n = n()) %>% 
  mutate(pct = n/sum(n),
         lbl = scales::percent(pct))
plotdata
```


```{r}
# create segmented bar chart
# adding labels to each segment

ggplot(plotdata, 
       aes(x = Attrition,
           y = pct,
           fill = BusinessTravel)) + 
  geom_bar(stat = "identity",
           position = "fill") +
  scale_y_continuous(breaks = seq(0, 1, .2),label = percent) +
  geom_text(aes(label = lbl), 
            size = 3, 
            position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette = "Set2") +
  #labs(y = "Percent",fill = "Drive Train",x = "Class",title = "Automobile Drive by Class") +
  theme_minimal()
```
# 3.1 Plot categorical vs catagorical

```{r}
data %>%
  dplyr::group_by(Attrition, BusinessTravel) %>%
  dplyr::summarize(n = n()) %>% 
  dplyr::mutate(pct = n/sum(n),lbl = scales::percent(pct)) %>% 
  ggplot(aes(x = Attrition,y = pct,
           fill = BusinessTravel)) + 
  geom_bar(stat = "identity",
           position = "fill") +
  scale_y_continuous(breaks = seq(0, 1, .2),label = percent) +
  geom_text(aes(label = lbl), 
            size = 3, 
            position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette = "Set2") +
  #labs(y = "Percent",fill = "Drive Train",x = "Class",title = "Automobile Drive by Class") +
  theme_minimal()
```

```{r}
Categorical_vs_categorical_plot <- function(data,group_col,fill_col){
data %>%
  group_by_(group_col, fill_col) %>%
  summarize(n = n()) %>% 
  mutate(pct = n/sum(n),lbl = scales::percent(pct))%>% 
  ggplot(aes_(x = group_col,y = ~pct,
           fill = fill_col)) +
  geom_bar(stat = "identity",
           position = "fill") +
  scale_y_continuous(breaks = seq(0, 1, .2),label = percent) +
  geom_text(aes(label = lbl), 
            size = 3, 
            position = position_stack(vjust = 0.5)) +
  #scale_fill_brewer(palette = "Set2") +
  labs(y = "Percent",x = "Attrition",title = "Compare attrition accross category")+
  theme_minimal()  
  
}
```

```{r}
# Get all character columns
data %>% 
  select_if(is.character)
```

```{r}
Categorical_vs_categorical_plot(data,~Attrition,~BusinessTravel)
Categorical_vs_categorical_plot(data,~Attrition,~Department)
Categorical_vs_categorical_plot(data,~Attrition,~EducationField)
Categorical_vs_categorical_plot(data,~Attrition,~Gender)
Categorical_vs_categorical_plot(data,~Attrition,~JobRole)
Categorical_vs_categorical_plot(data,~Attrition,~MaritalStatus)
Categorical_vs_categorical_plot(data,~Attrition,~OverTime)

```

# Combine multy plot
```{r}
#install.packages("patchwork")
library(patchwork)
p1 <- Categorical_vs_categorical_plot(data,~Attrition,~BusinessTravel)
p2 <- Categorical_vs_categorical_plot(data,~Attrition,~Department)
p3 <- Categorical_vs_categorical_plot(data,~Attrition,~EducationField)
```


```{r}
#install.packages("gridExtra")
library(gridExtra)
gridExtra::grid.arrange(p1,p2,p3)
```

# 3.2 Plot Quantitative vs. Quantitative

```{r}
str(data)
```


```{r}
# plot the distribution of salaries 
# by rank using kernel density plots
ggplot(data, 
       aes(x = DistanceFromHome, 
           fill = Attrition)) +
  geom_density(alpha = 0.4) 
  #labs(title = "Salary distribution by rank")
```

```{r}
#install.packages("ggridges")
library(ggridges)

ggplot(data, 
       aes(x = DistanceFromHome, 
           y = Attrition, 
           fill = Attrition)) +
  geom_density_ridges() + 
  theme_ridges() +
  theme(legend.position = "none")
```

```{r}
# plot the distribution of salaries by rank using boxplots
ggplot(data, 
       aes(x = Attrition, 
           y = DistanceFromHome)) +
  geom_boxplot() 
  #labs(title = "Salary distribution by rank")

# plot the distribution using violin and boxplots
ggplot(data, aes(x = Attrition, 
                     y = DistanceFromHome)) +
  geom_violin(fill = "cornflowerblue") +
  geom_boxplot(width = .2, 
               fill = "orange",
               outlier.color = "orange",
               outlier.size = 2) 
```
```{r}
ggplot(data, 
       aes(x = EnvironmentSatisfaction, 
           fill = Attrition)) +
  geom_density(alpha = 0.4) 
```


```{r}
ggplot(data, 
       aes(x = Attrition, 
           y = EnvironmentSatisfaction)) +
  geom_boxplot() 

# plot the distribution using violin and boxplots
ggplot(data, aes(x = Attrition, 
                     y = EnvironmentSatisfaction)) +
  geom_violin(fill = "cornflowerblue") +
  geom_boxplot(width = .2, 
               fill = "orange",
               outlier.color = "orange",
               outlier.size = 2) 

```
```{r}
ggplot(data, aes(x = Attrition, 
                     y = MonthlyIncome)) +
  geom_violin(fill = "cornflowerblue") +
  geom_boxplot(width = .2, 
               fill = "orange",
               outlier.color = "orange",
               outlier.size = 2) +
  facet_wrap(~Department) 
```
```{r}
ggplot(data, aes(x = Attrition, 
                     y = MonthlyIncome)) +
  geom_violin(fill = "cornflowerblue") +
  geom_boxplot(width = .2, 
               fill = "orange",
               outlier.color = "orange",
               outlier.size = 2) +
  facet_wrap(~JobRole) 
```
```{r}
ggplot(data, aes(x = Attrition, 
                     y = MonthlyIncome)) +
  geom_violin(fill = "cornflowerblue") +
  geom_boxplot(width = .2, 
               fill = "orange",
               outlier.color = "orange",
               outlier.size = 2) +
  facet_wrap(~JobSatisfaction) 
```
```{r}
ggplot(data, aes(x = Attrition, 
                     y = MonthlyIncome)) +
  geom_violin(fill = "cornflowerblue") +
  geom_boxplot(width = .2, 
               fill = "orange",
               outlier.color = "orange",
               outlier.size = 2) +
  facet_wrap(~PerformanceRating) 
```

```{r}
Categorical_vs_quantitative_plot <- function(data,categorical_col,quantitative_col){
  # plot the distribution using violin and boxplots
  ggplot(data, aes_(x = categorical_col, 
                     y = quantitative_col)) +
  geom_violin(fill = "cornflowerblue") +
  geom_boxplot(width = .2, 
               fill = "orange",
               outlier.color = "orange",
               outlier.size = 2) 
}
```

```{r}
data %>% 
  select_if(is.numeric)
```


```{r}
Categorical_vs_quantitative_plot(data,~Attrition,~Age)
Categorical_vs_quantitative_plot(data,~Attrition,~DistanceFromHome)
Categorical_vs_quantitative_plot(data,~Attrition,~Education)
Categorical_vs_quantitative_plot(data,~Attrition,~EnvironmentSatisfaction)
Categorical_vs_quantitative_plot(data,~Attrition,~HourlyRate)
Categorical_vs_quantitative_plot(data,~Attrition,~JobInvolvement)
Categorical_vs_quantitative_plot(data,~Attrition,~JobLevel)
Categorical_vs_quantitative_plot(data,~Attrition,~JobSatisfaction)
Categorical_vs_quantitative_plot(data,~Attrition,~MonthlyIncome)
Categorical_vs_quantitative_plot(data,~Attrition,~MonthlyRate)
Categorical_vs_quantitative_plot(data,~Attrition,~NumCompaniesWorked)
Categorical_vs_quantitative_plot(data,~Attrition,~PerformanceRating)
Categorical_vs_quantitative_plot(data,~Attrition,~RelationshipSatisfaction)
Categorical_vs_quantitative_plot(data,~Attrition,~TotalWorkingYears)
Categorical_vs_quantitative_plot(data,~Attrition,~WorkLifeBalance)
Categorical_vs_quantitative_plot(data,~Attrition,~YearsAtCompany)
Categorical_vs_quantitative_plot(data,~Attrition,~YearsInCurrentRole)
Categorical_vs_quantitative_plot(data,~Attrition,~YearsSinceLastPromotion)
Categorical_vs_quantitative_plot(data,~Attrition,~YearsWithCurrManager)
```
# 3.3 Quantitative vs. Quantitative
```{r}
# scatterplot with linear fit line
ggplot(data,
       aes(x = TotalWorkingYears, 
           y = MonthlyIncome)) +
  geom_point(color= "steelblue") +
  geom_smooth(method = "lm")
# Age vs monthly income
ggplot(data,
       aes(x = Age, 
           y = MonthlyIncome)) +
  geom_point(color= "steelblue") +
  geom_smooth(method = "lm")
```
```{r}
fit1 <- lm(MonthlyIncome ~ TotalWorkingYears, data = data)
summary(fit1)
```
# Correlation Matrix:
```{r}
# # Let's have a better understanding about each feature through a correlation plot
options(repr.plot.width=10, repr.plot.height=7) 
nums <- select_if(data, is.numeric)
```

```{r}
library(ggcorrplot)
corr <- round(cor(nums), 1)

ggcorrplot(corr, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="square", 
           colors = c("tomato2", "white", "#01A9DB"), 
           title="Correlogram Employee Attritions", 
           ggtheme=theme_minimal())
```

```{r}
library(superheat)
superheat(nums, scale = TRUE)
```
```{r}
# sorted heat map
superheat(nums,
          scale = TRUE,
          left.label.text.size=3,
          bottom.label.text.size=3,
          bottom.label.size = .05,
          row.dendrogram = TRUE )
```
create a scatterplot matrix (long and not good visual)
```{r}
# create a scatterplot matrix (long and not good visual)
#library(GGally)
#ggpairs(nums)
```
# Radar chart

```{r}
head(data)
```
```{r}
 data %>% select_if(function(col) is.numeric(col) | 
                                   all(col == .$Attrition))
```

```{r}
plotdata <- data %>% 
            select_if(function(col) is.numeric(col) | all(col == .$Attrition)) %>% 
            rename(group = Attrition) %>%
            mutate_at(vars(-group),funs(rescale)) %>% 
            relocate(group,Age)
plotdata
```
```{r}
library(ggradar)
library(scales)
ggradar::ggradar(plotdata)
```

# px.parallel_coordinates chart
```{r}
#install.packages("parcoords")
library(parcoords)
```

```{r}
data_parrallel_plot <-  data %>% 
                          select_if(is.character) %>% 
                          relocate(Attrition, .after = last_col())
data_parrallel_plot
```

```{r}
parcoords(
 data_parrallel_plot,
 reorderable = T,
 brushMode = '1D-axes')
```


```{r}
library(GGally)
#set the value of alpha to 0.5
ggparcoord(data_parrallel_plot, columns=1:8, groupColumn = 9, alphaLines = 0.5)
```

```{r}
```


```{r}
```


# 3.4 Select column type for data processing

```{r}
# Get all character columns
data %>% 
  select_if(is.character)
# select_if(~!is.numeric(.))
```
```{r}
# Get all character columns
data %>% 
  select_if(is.numeric)
# select_if(~!is.numeric(.))
```
```{r}
class(colnames(data))
```
```{r}
list(colnames(data))
```

```{r}
for (i in list(colnames(data))){
  print(i)
}
```

# 4. Machine learning (ISTA 321)
```{r}
library(rsample)   # for data splitting
library(caret)     # for model packages
```
```{r}
path="WA_Fn-UseC_-HR-Employee-Attrition.csv"
data <- fread(path)
str(data)
```
# Delete unecessary columns
```{r}
data_reduce <-data %>% 
              dplyr::select(-Over18,-EmployeeNumber, -EmployeeCount)

dim(data_reduce)
```
# Getting dummy variables - one hot encoding

First, let’s create our dummy object that we’ll use for the conversion. We’ll use the dummyVars() function from caret. Note how the formula is churn ~ . This is telling it not to convert churn as that’s our target, but look at all the other features (that’s what the period does) and convert if needed. I’ve also specified fullRank = TRUE so that it automatically drops one factor level which we want. The level dropped is the reference level then.

```{r}
data_reduce_dummy <- dummyVars(Attrition ~ ., data = data_reduce, fullRank = TRUE)
data_reduce_dummy
```

```{r}
data_reduce <- predict(data_reduce_dummy, newdata = data_reduce)
data_reduce <- data.frame(data_reduce)
str(data_reduce)
```

```{r}
data_reduce
```
```{r}
Attrition_vals <- data %>% 
  dplyr::select(Attrition)
data_reduce <- cbind(data_reduce, Attrition_vals) # attach it back
data_reduce
```
# Dealing with nuances in model preferences

```{r}
data_reduce$Attrition <-  factor(ifelse(data_reduce$Attrition == 'Yes', 1, 0))
```
# Dealing with nuances in my preferences

When we one hot encoded it took the various levels found in the columns and then created column names that used those levels. The problem is that those values when in the columns had spaces, which it turned into periods in the column names. I can’t deal with periods and underscores in the same column names.
```{r}
colnames(data_reduce)
```
Let’s use some regex to replace all periods with underscores. Remember that period is a special character so you need to escape it with two backslashes ‘\’. I’m also going to add a ‘+’ to the end of my search string as ‘+’ tells it to look for one or more of the symbols. This way it’ll grab the double period too!
```{r}
library(stringr)
colnames(data_reduce) <- colnames(data_reduce) %>% stringr::str_replace_all('\\.+', '_')
colnames(data_reduce) # check!
```
# 4.3 Splitting into training and test sets
The caret package has function that makes this really easy. You can feed the function createDataPartition() your target, and your split ratio and it’ll give you back a list of row numbers that you can use to randomly select 80% of the data. Let’s see it in action.
```{r}
# Method 2 From ISTA 321
data_split <- createDataPartition(data_reduce$Attrition, p = 0.8, list = FALSE)
dim(data_reduce)
dim(data_split)
head(data_split, 10)

```
So what’s in this ts_split object? Looking at the first 10 values we see that there is a continuous index but then under Resample1 we see that there are some randomly missing values… those are the 20% that weren’t selected!
```{r}
features_train <- data_reduce[ data_split, !(names(data_reduce) %in% c('Attrition'))] 
dim(features_train)
```
```{r}
features_test <- data_reduce[ -data_split, !(names(data_reduce) %in% c('Attrition'))] 
dim(features_test)
```


```{r}
target_train <- data_reduce[ data_split, "Attrition"]
target_test <- data_reduce[-data_split, "Attrition"]

#check propation target
table(target_train)
table(target_test)
prop.table(table(target_train))
prop.table(table(target_test))
```

#  Centering and scaling
We’ll use the preProcess() function to do our preprocessing. What’s great is that we can tell it all the things we want it to do at once within method = In this case we’ll tell it to center, scale, and even use KNN to impute missing values.

```{r}
preprocess_object <- preProcess(features_train, 
                                  method = c('center', 'scale', 'knnImpute'))
preprocess_object # We can look at the object and it'll tell us what it did
```
Now we can use ’predict()` to apply that preprocess object to our data (just like we used predict for creating our dummy variables).
```{r}
features_train <- predict(preprocess_object, newdata = features_train)
features_test <- predict(preprocess_object, newdata = features_test)
```
A quick histogram shows that Age has been scaled
```{r}
hist(features_train$Age)
```

# Fitting our models

Fitting a kNN model
```{r}
knn_fit <- knn3(features_train, target_train, k = 5)
knn_fit # just check it
```


```{r}
knn_pred <- predict(knn_fit, features_test, type = 'class' )
predictions <- cbind(data.frame(target_test, knn_pred))
summary(predictions)
```
```{r}
confusionMatrix(knn_pred, target_test)
```

Fitting a logistic regression model
```{r}
full_train <- cbind(features_train, target_train)
#glimpse(full_train)
```


```{r}
log_train <- glm(target_train ~ ., family = 'binomial', data = full_train)
summary(log_train)
```
Alright, now predict our test targets using our test features. Use type = 'response' so R knows to give you back the probability.

```{r}
log_pred <- predict(log_train, newdata = features_test, type = 'response')
head(log_pred) # just look at the first few values... note the 0-1 scale.
```
We can convert these probabilities to classes. In this case anything greater than or equal to 0.5 is a 1 and everything else a 0.

```{r}
log_pred <- ifelse(log_pred >= 0.5, 1, 0)
```

```{r}
predictions$log_pred <- factor(log_pred)
summary(predictions)
```


```{r}
confusionMatrix(factor(log_pred), target_test)
```

Fitting a linear discriminant analysis (LDA) model
```{r}
full_train[[37]]
```

```{r}
library(MASS) # Load mass - install if needed
lda_train <- lda(target_train ~ ., data = full_train) # fit model
lda_pred <- predict(lda_train, newdata = features_test, type = 'response') # predict
predictions$lda_pred <- lda_pred$class # add predictions to data frame
summary(predictions)
```

# 4. Mahicne learning model (ML BOOK)
```{r}
library(rsample)   # for data splitting
# Modeling packages
library(caret)     # for logistic regression modeling
# Model interpretability packages
library(vip)       # variable importance
```

```{r}
path="WA_Fn-UseC_-HR-Employee-Attrition.csv"
data <- fread(path)
dim(data)
```
Remove unnessary columns:
```{r}
data <-data %>%dplyr::select(-Over18,-EmployeeNumber, -EmployeeCount)
dim(data)
str(data)
```
Convert all character columns to factor
```{r}
# all character columns to factor:
data <- mutate_if(data, is.character, as.factor)
str(data)
```


```{r}
# Create training (70%) and test (30%) sets for the 
# rsample::attrition data.
set.seed(123)  # for reproducibility
churn_split <- initial_split(data, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
```
# Multiple logistic regression
# MLP khong su dung preprocess
```{r}
set.seed(123)
cv_model <- train(
  Attrition ~ ., 
  data = churn_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 5)
)
```


```{r}
summary(cv_model)
```


```{r}
# predict class
pred_class <- predict(cv_model, churn_train)
# create confusion matrix
confusionMatrix(
  data = pred_class, 
  reference = churn_train$Attrition
)
# NOTE: dung theo kieu nay khong duoc do hieu nham positive class la No, sensitivity, specificity, pos , neg lon nguoc so voi binh thuong
```
```{r}
pred_class <- predict(cv_model, churn_train)
# create confusion matrix (pp chuan phai relevel)
confusionMatrix(
  data = relevel(pred_class, ref = "Yes"), 
  reference = relevel(churn_train$Attrition, ref = "Yes")
)
```


```{r}
library(ROCR)
# Compute predicted probabilities
m1_prob <- predict(cv_model, churn_train, type = "prob")$Yes

# Compute AUC metrics for cv_model1 and cv_model3
perf1 <- prediction(m1_prob, churn_train$Attrition) %>%
  performance(measure = "tpr", x.measure = "fpr")

# Plot ROC curves for cv_model1 and cv_model3
plot(perf1, col = "black", lty = 2)

```


```{r}
vip(cv_model, num_features = 20)
```
Note: Neu chuyen cac column Job satisfaction tu int (1,2,3,4) thanh factor (low, normal, high) thi important feature explain se dien giai chinh xac column + factor tuong ung, vd: Job satisfaction Very_high, Job satisfaction _ Normal 

# MLP Su dung preprocess 
```{r}
set.seed(123)
cv_model <- train(
  Attrition ~ ., 
  data = churn_train, 
  method = "glm",
  family = "binomial",
  preProcess = c("zv", "center", "scale"), # loai bo near zero/constant column, center and scale column value
  trControl = trainControl(method = "cv", number = 5)
)
```

```{r}
pred_class <- predict(cv_model, churn_train)
# create confusion matrix (pp chuan phai relevel)
confusionMatrix(
  data = relevel(pred_class, ref = "Yes"), 
  reference = relevel(churn_train$Attrition, ref = "Yes")
)
```
```{r}
library(ROCR)
# Compute predicted probabilities
m1_prob <- predict(cv_model, churn_train, type = "prob")$Yes

# Compute AUC metrics for cv_model1 and cv_model3
perf1 <- prediction(m1_prob, churn_train$Attrition) %>%
  performance(measure = "tpr", x.measure = "fpr")

# Plot ROC curves for cv_model1 and cv_model3
plot(perf1, col = "black", lty = 2)
```


```{r}
vip(cv_model, num_features = 20)
```

# XGBoost
```{r}
# Read data
path="WA_Fn-UseC_-HR-Employee-Attrition.csv"
data <- fread(path)
dim(data)
# Remove unnessary columns:
data <-data %>%dplyr::select(-Over18,-EmployeeNumber, -EmployeeCount)
# all character columns to factor:
data <- mutate_if(data, is.character, as.factor)
# Create training (70%) and test (30%) sets for the 
# rsample::attrition data.
set.seed(123)  # for reproducibility
churn_split <- initial_split(data, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
dim(churn_train)
dim(churn_test)
```
```{r}
head(churn_train)
```

```{r}
library(xgboost)
```

```{r}
# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=data.matrix(churn_train),label=churn_train$Attrition)
xgb.test = xgb.DMatrix(data=data.matrix(churn_test),label=churn_test$Attrition)
```


```{r}
# Define the parameters for multinomial classification
num_class = length(levels(churn_test$Attrition))
params = list(
  booster="gbtree",
  eta=0.001,
  max_depth=5,
  gamma=3,
  subsample=0.75,
  colsample_bytree=1,
  objective="binary:logistic",
  eval_metric="mlogloss",
  num_class=num_class
)
```

```{r}
# Train the XGBoost classifer
xgb.fit=xgb.train(
  params=params,
  data=xgb.train,
  nrounds=25,
  nthreads=1,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=0
)

# Review the final model and results
xgb.fit
```
# 5 ML Follow statistic machine learning BOOK

```{r}
# Read data
path="WA_Fn-UseC_-HR-Employee-Attrition.csv"
data <- fread(path)
dim(data)
# Remove unnessary columns:
data <-data %>%dplyr::select(-Over18,-EmployeeNumber, -EmployeeCount)
# all character columns to factor:
data <- mutate_if(data, is.character, as.factor)
# Create training (70%) and test (30%) sets for the 
# rsample::attrition data.
set.seed(123)  # for reproducibility
churn_split <- initial_split(data, prop = .8, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
dim(churn_train)
dim(churn_test)
```
At first glance, it might appear as if the use of createDataPartition() is no different than our previous use of sample(). However, createDataPartition() tries to ensure a split that has a similar distribution of the supplied variable in both datasets. See the documentation for details.
```{r}
set.seed(430)
churn_split = createDataPartition(data$Attrition, p =0.8, list = FALSE)
churn_train = data[default_idx, ]
churn_test = data[-default_idx, ]
dim(churn_train)
dim(churn_test)
```
# Simple additive logistic regression.

```{r}
glm_mod = train(
  form = Attrition ~ .,
  data = churn_train,
  trControl = trainControl(method = "cv", number = 5),
  method = "glm",
  family = "binomial"
)
glm_mod
```

```{r}
#summary(glm_mod)
```

```{r}
pred_class <- predict(glm_mod, churn_test)
# create confusion matrix (pp chuan phai relevel)
confusionMatrix(
  data = relevel(pred_class, ref = "Yes"), 
  reference = relevel(churn_test$Attrition, ref = "Yes")
)
```
# KNN
```{r}
knn_mod = train(
  Attrition ~ .,
  data = churn_train,
  method = "knn",
  trControl = trainControl(method = "cv", number = 5)
)
knn_mod
```
Here we are again using 5-fold cross-validation and no pre-processing. Notice that we now have multiple results, for k = 5, k = 7, and k = 9.

Let’s modifying this training by introducing pre-processing, and specifying our own tuning parameters, instead of the default values above.

Perprocess nzv: For some other models, this might be an issue (especially if we resample or down-sample the data). We can add a filter to check for zero- or near zero-variance predictors prior to running the pre-processing calculations:
```{r}
knn_mod = train(
  Attrition ~ .,
  data = churn_train,
  method = "knn",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("nzv","center", "scale"),
  tuneGrid = expand.grid(k = seq(1, 101, by = 2))
)
```


```{r}
head(knn_mod$results, 5)
```


```{r}
plot(knn_mod)
```


```{r}
knn_mod$bestTune
```


```{r}
pred_class <- predict(knn_mod, churn_test)
# create confusion matrix (pp chuan phai relevel)
confusionMatrix(
  data = relevel(pred_class, ref = "Yes"), 
  reference = relevel(churn_test$Attrition, ref = "Yes")
)
```


```{r}

```


```{r}
```


```{r}
```

