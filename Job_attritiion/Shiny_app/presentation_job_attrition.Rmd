---
title: "Job attrition analytyic"
author: "Nguyen Ngoc Duy"
date: "Jan 9, 2021"
output: ioslides_presentation
---

# Some information

- Github link to shiniapp code, data file and presentation file:
https://github.com/duybluemind1988/datasciencecoursera/tree/main/9.%20Develop%20Data%20product/Job_attrition

- Job attrition instroduction:
Uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. This is a fictional data set created by IBM data scientists.

- Link to Kaggle job attrition information:
https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset

```{r message = FALSE,warning = FALSE,include=FALSE}
library(data.table)
library(tidyverse)
library(rsample)   # for data splitting
library(h2o)
library(caret)
h2o.no_progress()
h2o.init()
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
path <- "WA_Fn-UseC_-HR-Employee-Attrition.csv"
data <- fread(path)
head(data)
```
# Plot categorical vs catagorical
Compare Attrition percentage between business travel categorical
```{r}
Categorical_vs_categorical_plot <- function(data,group_col,fill_col){
data %>%
  group_by_(group_col, fill_col) %>%
  summarize(n = n()) %>% 
  mutate(pct = n/sum(n),lbl = scales::percent(pct))%>% 
  ggplot(aes_(x = group_col,y = ~pct,
           fill = fill_col)) +
  geom_bar(stat = "identity",
           position = "fill") +
  scale_y_continuous(breaks = seq(0, 1, .2),label =scales::percent) +
  geom_text(aes(label = lbl), 
            size = 3, 
            position = position_stack(vjust = 0.5)) +
  #scale_fill_brewer(palette = "Set2") +
  labs(y = "Percent",x = "Attrition",title = "Compare attrition accross category")+
  theme_minimal()  
  
}
Categorical_vs_categorical_plot(data,~Attrition,~BusinessTravel)
```
# Plot Categorical vs. Quantitative
```{r}
Categorical_vs_quantitative_plot <- function(data,categorical_col,quantitative_col){
  # plot the distribution using violin and boxplots
  ggplot(data, aes_(x = categorical_col, 
                     y = quantitative_col)) +
  geom_violin(fill = "cornflowerblue") +
  geom_boxplot(width = .2, 
               fill = "orange",
               outlier.color = "orange",
               outlier.size = 2) 
}
Categorical_vs_quantitative_plot(data,~Attrition,~Age)
#Categorical_vs_quantitative_plot(data,~Attrition,~DistanceFromHome)
#Categorical_vs_quantitative_plot(data,~Attrition,~Education)

```
```{r}
facet_plot <- function(data,categorical_col,quantitative_col,facet_col){
  # plot the distribution using violin and boxplots
  ggplot(data, aes_(x = categorical_col, 
                     y = quantitative_col)) +
  geom_violin(fill = "cornflowerblue") +
  geom_boxplot(width = .2, 
               fill = "orange",
               outlier.color = "orange",
               outlier.size = 2) +
  facet_wrap(facet_col) +
  labs(title=facet_col)
}

facet_plot(data,~Attrition,~MonthlyIncome,~Department)
#facet_plot(data,~Attrition,~MonthlyIncome,~JobRole)
#facet_plot(data,~Attrition,~MonthlyIncome,~JobSatisfaction)
#facet_plot(data,~Attrition,~MonthlyIncome,~PerformanceRating)
```


# Train model

```{r}
#data$Attrition<-ifelse(data$Attrition=="Yes", 1, 0)
# Create training (80%) and test (20%) sets for the 
set.seed(430)
split = createDataPartition(data$Attrition, p =0.8, list = FALSE)
train = data[split, ]
test = data[-split, ]
```
Convert to factor
```{r}
# all character columns to factor:
train <- mutate_if(train, is.character, as.factor)
```

Clean the Near Zero Variance Variables.
```{r}
#column_near_zero_var <-nearZeroVar(train)
#column_near_zero_var
```

```{r}
#train <- train[,-..column_near_zero_var]
```

With tree method, no need to standar scaler, remove null value, encoder (due to factor transform)...
```{r}
test <- mutate_if(test, is.character, as.factor)
#test <- test[,-..column_near_zero_var]
```

```{r}
# convert training data to h2o object
train_h2o <- as.h2o(train)
test_h2o <- as.h2o(test)
# set the response column to Sale_Price
response <- "Attrition"
n_features <- length(setdiff(names(train), "Attrition"))
# set the predictor names
predictors <- setdiff(colnames(train), response)
```

```{r}
start_time <- lubridate::minute(Sys.time())
h2o_model <- h2o.gbm(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    nfolds=5,
    seed = 123
)
end_time <- lubridate::minute(Sys.time())
#h2o_model
time <- (end_time - start_time)
print(time)
```
```{r}
# Apply h2o model to test set
test_pred <-h2o.predict(h2o_model, newdata = test_h2o)
h2o.performance(h2o_model,test_h2o)
```
https://groups.google.com/g/h2ostream/c/TkNkMFprzf0

h2o.predict uses .5 threshold for class prediction.

h2o.performance uses the threshold that maximizes F1 by default.

these thresholds are not in general the same.

```{r}
reference <- as.data.frame(test_h2o)$Attrition
predict <- as.data.frame(test_pred)$predict
table(reference)
table(predict)
# Confusion matrix, number of cases
table(reference, predict)
```

```{r}
library(mltools)
print("mcc")
mltools::mcc(predict,reference) # 0.4520577
#library(MLmetrics)
#install.packages("MLmetrics")
#result <- data_frame( reference,predict)
#colnames(result) <-c("obs","pred")
#result (NOT accuracy - wrong)
#print("precision")
#caret::precision(predict,reference) # Neg Pred value 0.91393
#print("recall")
#caret::recall(predict,reference) # Specificity 0.9065
#print("prSummary")
#caret::prSummary(result,lev = levels(result$obs))
#print("F1_Score")
#MLmetrics::F1_Score(predict,reference) # 0.910204
#print("AUC")
#MLmetrics::AUC(predict,reference)
```

```{r}
caret::confusionMatrix(predict,reference,positive = "Yes",mode="prec_recall")
```
# Try to improve machine learning model with recipe
```{r}
library(data.table)
library(tidyverse)
library(rsample)   # for data splitting
library(h2o)
library(caret)
library(recipes)
library(tictoc)
```

```{r}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
path <- "WA_Fn-UseC_-HR-Employee-Attrition.csv"
data <- fread(path)
head(data)
```
```{r}
split = createDataPartition(data$Attrition, p =0.8, list = FALSE)
train = data[split, ]
test = data[-split, ]
```


```{r}
# Convert character to factor ? (dummy ?)
# Convert numeric to factor ? 
recipe_obj <- recipe(Attrition ~ ., data = train) %>%
  step_nzv(all_numeric(), -all_outcomes())  %>% #Remove near-zero variance features like sex, yes/no...
  #step_knnimpute(all_predictors(), neighbors = 6) %>%  # Impute, very slow in large data in train, need step outside
  step_YeoJohnson(all_numeric(),-all_outcomes()) %>% # Remove skewness
  #step_integer(matches("Qual|Cond|QC|Qu")) %>% # Ordinal encoder
  step_center(all_numeric(), -all_outcomes()) %>% # center 
  step_scale(all_numeric(), -all_outcomes()) %>% # scale
  #step_dummy(all_nominal(), one_hot = TRUE) %>% 
  #step_pca(all_numeric(), -all_outcomes()) #Perform dimension reduction
  prep()

baked_train <- bake(recipe_obj, new_data = train)
baked_test <- bake(recipe_obj, new_data = test)
baked_train
```


```{r}
# convert training data to h2o object
train_h2o <- as.h2o(baked_train)
test_h2o <- as.h2o(baked_test)
# set the response column to Sale_Price
response <- "Attrition"
n_features <- length(setdiff(names(baked_train), "Attrition"))
# set the predictor names
predictors <- setdiff(colnames(baked_train), response)
```


```{r}
tic()
h2o_model <- h2o.gbm(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    nfolds=5,
    seed = 123
)
end_time <- lubridate::minute(Sys.time())
toc()
```


```{r}
test_pred <-h2o.predict(h2o_model, newdata = test_h2o)
reference <- as.data.frame(test_h2o)$Attrition
predict <- as.data.frame(test_pred)$predict
caret::confusionMatrix(predict,reference,positive = "Yes",mode="prec_recall")
```
```{r}
rocgbm <- roc(as.numeric(reference), as.numeric(predict))
rocgbm$auc
```

# Auto ML H2O
```{r}
auto_ml <- h2o.automl(
    x = predictors, 
    y = response,
    training_frame = train_h2o,
    #leaderboard_frame = h2o_validation,
    project_name = "Attrition",
    #max_runtime_secs = 300,
    max_models = 10,
    seed = 12
)
```


```{r}
# Check for the top models
top_models <- auto_ml@leaderboard
print(top_models)
```
```{r}
test_pred <- h2o.predict(auto_ml, test_h2o) 
reference <- as.data.frame(test_h2o)$Attrition
predict <- as.data.frame(test_pred)$predict
caret::confusionMatrix(predict,reference,positive = "Yes",mode="prec_recall")
```

# Try ML model with 90 acc
https://www.kaggle.com/esmaeil391/ibm-hr-analysis-with-90-3-acc-and-89-auc
```{r}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
path <- "WA_Fn-UseC_-HR-Employee-Attrition.csv"
data <- fread(path)
head(data)
```
```{r}
# Convert char/string to factor
data <- mutate_if(data, is.character, as.factor)
# Remove non nessearry column:
data$EmployeeNumber<- NULL
data$StandardHours <- NULL
data$Over18 <- NULL
data$EmployeeCount <- NULL
# Convert numeric value to factor
data$Education <- factor(data$Education)
data$EnvironmentSatisfaction <- factor(data$EnvironmentSatisfaction)
data$JobInvolvement <- factor(data$JobInvolvement)
data$JobLevel <- factor(data$JobLevel)
data$JobSatisfaction <- factor(data$JobSatisfaction)
data$PerformanceRating <- factor(data$PerformanceRating)
data$RelationshipSatisfaction <- factor(data$RelationshipSatisfaction)
data$StockOptionLevel <- factor(data$StockOptionLevel)
data$WorkLifeBalance <- factor(data$WorkLifeBalance)

head(data)
```

Split way 1: Wrong because not split by percentage minor group, if minor group is low, accuracy with be higher

rfData <- data
set.seed(123)
indexes = sample(1:nrow(rfData), size=0.8*nrow(rfData))
RFRaw.train.Data <- rfData[indexes,]
RFRaw.test.Data <- rfData[-indexes,]


Split way 2:

```{r}
split = createDataPartition(data$Attrition, p =0.8, list = FALSE)
train = data[split, ]
test = data[-split, ]
```

```{r}
library(randomForest)
Raw.rf.model <- randomForest(Attrition~.,RFRaw.train.Data, importance=TRUE,ntree=1000)
```

```{r}
predict <- predict(Raw.rf.model, newdata = RFRaw.test.Data)
reference <- RFRaw.test.Data$Attrition
confusionMatrix( predict, reference ,positive="Yes",mode="prec_recall")
```

```{r}
test_pred <- predict(Raw.rf.model, test) 
reference <- test$Attrition
predict <- test_pred
caret::confusionMatrix(predict,reference,positive = "Yes",mode="prec_recall")
```


```{r}
```


```{r}
```

# save the model
```{r}
getwd()
```

```{r}
#model_path <- h2o.saveModel(object = h2o_model, path = getwd(), force = TRUE)
#print(model_path)
```

# Load the model

```{r}
# load the model
model_path <- "GBM_model_R_1612186807441_436"
h2o_model <- h2o.loadModel(model_path)
```

# Explain model
```{r}
## Variable important
h2o.varimp_plot(h2o_model)
```
```{r}
## Shap explain model
h2o.shap_summary_plot(h2o_model,test_h2o)
```
```{r}
h2o.pd_plot(h2o_model, test_h2o, column = "OverTime")
h2o.pd_plot(h2o_model, test_h2o, column = "MonthlyIncome")
h2o.pd_plot(h2o_model, test_h2o, column = "StockOptionLevel")
h2o.pd_plot(h2o_model, test_h2o, column = "DistanceFromHome")
h2o.pd_plot(h2o_model, test_h2o, column = "EnvironmentSatisfaction")
```
```{r}
h2o.ice_plot(h2o_model, test_h2o, column = "MonthlyIncome")
```
```{r}
# Class = No
h2o.shap_explain_row_plot(h2o_model, test_h2o,row_index = 10)
h2o.shap_explain_row_plot(h2o_model, test_h2o,row_index = 4)
```


```{r}
# Class =Yes #2,17,20
h2o.shap_explain_row_plot(h2o_model, test_h2o,row_index = 18)
h2o.shap_explain_row_plot(h2o_model, test_h2o,row_index = 3)
```

# Create data for predict

```{r}
dim(data)
```

```{r}
num_input <- 2 # how many data for predict ?
employ.data <- data.frame(matrix(ncol = dim(data)[2], nrow = num_input))
names(employ.data) <- colnames(data)
employ.data
```

```{r}
employ.data$MonthlyIncome <- c(2000,5000) 
employ.data$OverTime <- c("Yes","No")
employ.data$DailyRate <- c(1000,591)
employ.data$MonthlyRate <- c(9964,1000)
employ.data$DistanceFromHome <- c(24,5)
employ.data$Age <- c(25,40)
employ.data$StockOptionLevel <- c(1,3)
employ.data$RelationshipSatisfaction <- c(1,3)
```

```{r}
employ.data <- mutate_if(employ.data, is.character, as.factor)
#employ.data <- employ.data[,-..column_near_zero_var]
employ.data <- as.h2o(employ.data)
```

```{r}
pred <-h2o.predict(h2o_model, newdata = employ.data)
pred
```
```{r}
h2o.shap_explain_row_plot(h2o_model, employ.data,row_index = 1)
h2o.shap_explain_row_plot(h2o_model, employ.data,row_index = 2)
```

