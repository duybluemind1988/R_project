---
title: "Credit_card"
output: html_document
---

The datasets contains transactions made by credit cards in September 2013 by european cardholders.
This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.

It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.
```{r}
library(tidyverse)
library(data.table)
library(caret)
library(recipes) # create preprocess ml pipeline
```

```{r}
getwd()
```

```{r}
#path='/media/ad/01D6B57CFBE4DB20/1.Linux/Data/FQC/V04R-V04R-SQLData_4000_tail.dat'
path="/media/ad/01D6B57CFBE4DB20/1.Linux/Data/creditcard.csv"
data <- fread(path)
data
```
# Check null value

```{r}
sum(is.na(data))
```
No null value in this dataset

```{r}
library(base)
base::table(data$Class)
```


```{r}
library(DataExplorer)
```


```{r}
## View basic description for airquality data
introduce(data)
```


```{r}
## Plot basic description for airquality data
plot_intro(data)
```

```{r}
## View missing value distribution for airquality data
plot_missing(data)
```


```{r}
## Left: frequency distribution of all discrete variables
plot_bar(data)
```


```{r}
## View histogram of all continuous variables
plot_histogram(data)
```


```{r}
## View estimated density distribution of all continuous variables
plot_density(data)
```


```{r}
## View quantile-quantile plot of all continuous variables
#plot_qq(data)
```

```{r}
corr_matrix <- cor(data %>% select_if(is.numeric))
corr_matrix <- round(corr_matrix,2)
class(corr_matrix)
corr_matrix_df <- as.data.frame(apply(corr_matrix, 2, unlist)) %>% select(Class) %>% arrange(desc(Class))
# Name the unnamed first column
corr_matrix_df <- cbind(Features= rownames(corr_matrix_df), corr_matrix_df)
rownames(corr_matrix_df) <- NULL
corr_matrix_df
```
```{r}
# Top 10 for correlation
top_10 <-corr_matrix_df[1:11,1]
top_20 <-corr_matrix_df[1:21,1]
class(top_20)
top_20
```
```{r}
## View overall correlation heatmap
plot_correlation(data)
```

```{r}
library(dlookr)
plot_correlate(data)
```

```{r}
Categorical_vs_quantitative_plot <- function(data,categorical_col,quantitative_col){
  # plot the distribution using violin and boxplots
  ggplot(data, aes_(x = categorical_col, 
                     y = quantitative_col)) +
  geom_violin(fill = "cornflowerblue") +
  geom_boxplot(width = .2, 
               fill = "orange",
               outlier.color = "orange",
               outlier.size = 2) 
}
```

```{r}
corr_matrix_df
```


```{r}
Categorical_vs_quantitative_plot(data,~factor(Class),~V11)
Categorical_vs_quantitative_plot(data,~factor(Class),~V4)
Categorical_vs_quantitative_plot(data,~factor(Class),~V2)
Categorical_vs_quantitative_plot(data,~factor(Class),~V12)
Categorical_vs_quantitative_plot(data,~factor(Class),~V14)
Categorical_vs_quantitative_plot(data,~factor(Class),~V17)
```

# Machine learning model
## Prepare data for machine learning
```{r}
library(tidyverse)
library(data.table)
library(caret)
library(recipes) # create preprocess ml pipeline
```


```{r}
#path='/media/ad/01D6B57CFBE4DB20/1.Linux/Data/FQC/V04R-V04R-SQLData_4000_tail.dat'
path="/media/ad/01D6B57CFBE4DB20/1.Linux/Data/creditcard.csv"
data <- fread(path)
```

```{r}
# all character columns to factor:
data <- mutate_if(data, is.character, as.factor)
data$Class <-as.factor(data$Class)
#data$Attrition<-ifelse(data$Attrition=="Yes", 1, 0)
# Create training (80%) and test (20%) sets for the 
set.seed(430)
split = createDataPartition(data$Class, p =0.8, list = FALSE)
train = data[split, ]
test = data[-split, ]
dim(train)
dim(test)
```

While your project’s needs may vary, here is a suggested order of potential steps that should work for most problems:

Filter out zero or near-zero variance features.
Perform imputation if required.
Normalize to resolve numeric feature skewness.
Standardize (center and scale) numeric features.

```{r}
blueprint <- recipe(Class ~ ., data = train) %>%
  step_nzv(all_nominal(),-all_outcomes())  %>% 
  #step_knnimpute(all_predictors(), neighbors = 6) %>%  # Impute, very slow in large data in train, need step outside
  step_YeoJohnson(all_numeric(),-all_outcomes()) %>% # Remove skewness
  step_center(all_numeric(), -all_outcomes()) %>% # center 
  step_scale(all_numeric(), -all_outcomes()) #%>% # scale
  #step_dummy(all_nominal(), one_hot = TRUE) %>% 
  #step_pca(all_numeric(), -all_outcomes()) #Perform dimension reduction
blueprint
```
Next, we need to train this blueprint on some training data. Remember, there are many feature engineering steps that we do not want to train on the test data (e.g., standardize and PCA) as this would create data leakage. So in this step we estimate these parameters based on the training data of interest.

```{r}
prepare <- prep(blueprint, training = train)
prepare
```
Lastly, we can apply our blueprint to new data (e.g., the training data or future test data) with bake()

```{r}
baked_train <- bake(prepare, new_data = train)
baked_test <- bake(prepare, new_data = test)
head(baked_train)
```
6. Check mulity caret ML model

```{r}
#library(caret) # logistic regression, lm, knn, 
#library(glmnet)   # for implementing regularized regression
#library(earth)     # for fitting MARS models
#library(rpart) # decision tree
#library(ipred)       # for fitting bagged decision trees
#library(ranger)   # a c++ implementation of random forest 

library(doParallel)
cl <-makePSOCKcluster(5)
registerDoParallel(cl)
```
```{r}
list_model <- c("knn","rpart","lda","svmRadial","ranger","LogitBoost","naive_bayes","gbm","gbm_h2o","glmnet_h2o")
```

6 GB for 100 MB data train by lda, very long
4 GB for gbm model
```{r}
metric <- "Accuracy"
model_name <- "gbm" # Stochastic Gradient Boosting
start_time <- lubridate::second(Sys.time())
model <- train(Class~., data=baked_train, 
               method=model_name, metric=metric)
end_time <- lubridate::second(Sys.time())
time <- (end_time - start_time)
print(time)
```


```{r}
metric <- "Accuracy"
model_name <- "mlpSGD" # Multilayer Perceptron Network by Stochastic Gradient Descent
start_time <- Sys.time()
model <- train(Class~., data=baked_train, 
               method=model_name, metric=metric)
end_time <- Sys.time()
time <- (end_time - start_time)
print(time)
```

# H2O machine learning model
```{r}
library(h2o)
h2o.no_progress()
h2o.init()
```


```{r}
# convert training data to h2o object
train_h2o <- as.h2o(baked_train)
test_h2o <- as.h2o(baked_test)
# set the response column to Sale_Price
response <- "Class"
n_features <- length(setdiff(names(baked_train), "Class"))
# set the predictor names
predictors <- setdiff(colnames(baked_train), response)
```

XGboost H2O

```{r}
start_time <- Sys.time()
h2o_xgboost <- h2o.xgboost(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    seed = 123
)
end_time <- Sys.time()
h2o_xgboost
#pred <-h2o.predict(h2o_xgboost, newdata = test_h2o)
time <- (end_time - start_time)
print(time) # 57 s mcc 0.84

```


```{r}
h2o.performance(h2o_xgboost, newdata = test_h2o) # mcc 0.84
```

Gradient Boosting Machine (GBM)

```{r}
n_features
```

```{r}
start_time <- Sys.time()
h2o_gbm <- h2o.gbm(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    ntrees = n_features * 10, # very slow if high dim, not pca
    #learn_rate=0.2,
    seed = 123
)
end_time <- Sys.time()
h2o_gbm
time <- (end_time - start_time)
print(time) # 1.79 mins mcc: 0.48 (gan voi f1)
```


```{r}
#pred <-h2o.predict(h2o_gbm, newdata = test_h2o)
perf <-h2o.performance(h2o_gbm, newdata = test_h2o) # mcc: 0.48 (gan voi f1)
perf
```
```{r}
h2o.confusionMatrix(perf)
```

Generalized Linear Models (GLM)

```{r}
start_time <- Sys.time()
print(start_time)
h2o_glm <- h2o.glm(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    #lambda_search = TRUE,
    seed = 123
)
end_time <- Sys.time()
print(end_time)
h2o_glm
pred <-h2o.predict(h2o_glm, newdata = test_h2o)
time <- (end_time - start_time)
print(time) # 3.79 s mcc 0.755
```

```{r}
h2o.performance(h2o_glm, newdata = test_h2o) # mcc 0.755
```

Random forest
```{r}
start_time <- Sys.time()
print(start_time)
h2o_rf1 <- h2o.randomForest(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    ntrees = n_features * 10, # very slow if high dim, not pca
    #ntrees=300,
    seed = 123
)
end_time <- Sys.time()
print(end_time)
h2o_rf1
pred <-h2o.predict(h2o_rf1, newdata = test_h2o)
time <- (end_time - start_time)
print(time) # 2.1 mins mcc 0.87
```

```{r}
h2o.performance(h2o_rf1, newdata = test_h2o)
```
Note: 
Perofrmance: 1. ranger - XGboost - GLM - GBM
Speed:        1. GLM - XGboost - GBM - ranger

# Explain model

```{r}
perf <- h2o.performance(h2o_xgboost, newdata = test_h2o)
plot(perf, type = "roc")
```


```{r}
# Explain a model
exm <- h2o.explain(h2o_xgboost, test_h2o)
exm
```
```{r}
h2o.varimp_plot(h2o_xgboost)
```

```{r}
h2o.shap_explain_row_plot(h2o_xgboost, test_h2o,row_index = 1)
h2o.shap_explain_row_plot(h2o_xgboost, test_h2o,row_index = 2)
```

```{r}
h2o.shap_summary_plot(h2o_xgboost,test_h2o)
```


```{r}
h2o.pd_plot(h2o_xgboost, test_h2o, column = "V17")
```


```{r}
h2o.ice_plot(h2o_xgboost, test_h2o, column = "V17")
```
# Autoencoders and anomaly detection with machine learning in fraud analytics
https://shiring.github.io/machine_learning/2017/05/01/fraud
```{r}
library(tidyverse)
library(data.table)
```

```{r}
# path="/media/ad/01D6B57CFBE4DB20/1.Linux/Data/creditcard.csv"
path="C:/Users/DNN/Data_science/Git/creditcard.csv"
creditcard <- fread(path)
```

```{r}
creditcard %>%
  ggplot(aes(x = Class)) +
    geom_bar() 
```


```{r}
summary(creditcard$Time)
```


```{r}
# convert class variable to factor
creditcard$Class <- factor(creditcard$Class)
```


```{r}
creditcard %>%
  ggplot(aes(x = Amount)) +
    geom_histogram(color = "grey", fill = "lightgrey", bins = 50) +
    theme_bw() +
    facet_wrap( ~ Class, scales = "free", ncol = 2)
```
Interestingly, fraudulent credit card transactions had a higher mean amount of money that was transferred, but the maximum amount was much lower compared to regular transactions.

```{r}
library(h2o)
h2o.init()
```


```{r}
# convert data to H2OFrame
creditcard_hf <- as.h2o(creditcard)
```

```{r}
splits <- h2o.splitFrame(creditcard_hf, 
                         ratios = c(0.4, 0.4), 
                         seed = 42)

train_unsupervised  <- splits[[1]]
train_supervised  <- splits[[2]]
test <- splits[[3]]

response <- "Class"
features <- setdiff(colnames(train_unsupervised), response)
```

```{r}
dim(train_unsupervised)
dim(train_supervised)
dim(test)
```
```{r}
table(as.data.frame(train_unsupervised["Class"]))
table(as.data.frame(train_supervised["Class"]))
table(as.data.frame(test["Class"]))
```

```{r}
prop.table(table(as.data.frame(train_unsupervised["Class"])))
prop.table(table(as.data.frame(train_supervised["Class"])))
prop.table(table(as.data.frame(test["Class"])))
```

Autoencoders

First, I am training the unsupervised neural network model using deep learning autoencoders. With h2o, we can simply set autoencoder = TRUE.

Here, I am applying a technique called “bottleneck” training, where the hidden layer in the middle is very small. This means that my model will have to reduce the dimensionality of the input data (in this case, down to 2 nodes/dimensions).

The autoencoder model will then learn the patterns of the input data irrespective of given class labels. Here, it will learn, which credit card transactions are similar and which transactions are outliers or anomalies. We need to keep in mind though, that autoencoder models will be sensitive to outliers in our data, which might throw off otherwise typical patterns.
```{r}
model_nn <- h2o.deeplearning(x = features,
                             training_frame = train_unsupervised,
                             model_id = "model_nn",
                             autoencoder = TRUE,
                             reproducible = TRUE, #slow - turn off for real problems
                             ignore_const_cols = FALSE,
                             seed = 42,
                             hidden = c(10, 2, 10), 
                             epochs = 100,
                             activation = "Tanh")
```


```{r}
h2o.saveModel(model_nn, path="model_nn", force = TRUE)
```


```{r}
model_nn <- h2o.loadModel("model_nn/model_nn")
model_nn
```

Dimensionality reduction with hidden layers

Because I had used a bottleneck model with two nodes in the hidden layer in the middle, we can use this dimensionality reduction to explore our feature space (similar to what to we could do with a principal component analysis). We can extract this hidden feature with the h2o.deepfeatures() function and plot it to show the reduced representation of the input data.
```{r}
train_features <- h2o.deepfeatures(model_nn, train_unsupervised, layer = 2) %>%
  as.data.frame() %>%
  mutate(Class = as.vector(train_unsupervised[, 31]))
train_features
```


```{r}
ggplot(train_features, aes(x = DF.L2.C1, y = DF.L2.C2, color = Class)) +
  geom_point(alpha = 0.1)
```

Here, we do not see a cluster of fraudulent transactions that is distinct from non-fraud instances, so dimensionality reduction with our autoencoder model alone is not sufficient to identify fraud in this dataset.

But we could use the reduced dimensionality representation of one of the hidden layers as features for model training. An example would be to use the 10 features from the first or third hidden layer:
```{r}
# let's take the third hidden layer
train_features <- h2o.deepfeatures(model_nn, train_unsupervised, layer = 3) %>%
  as.data.frame() %>%
  mutate(Class = as.factor(as.vector(train_unsupervised[, 31]))) %>%
  as.h2o()
```


```{r}
train_features
dim(train_features)
```


```{r}
features_dim <- setdiff(colnames(train_features), response)
```


```{r}
model_nn_dim <- h2o.deeplearning(y = response,
                               x = features_dim,
                               training_frame = train_features,
                               reproducible = TRUE, #slow - turn off for real problems
                               balance_classes = TRUE,
                               ignore_const_cols = FALSE,
                               seed = 42,
                               hidden = c(10, 2, 10), 
                               epochs = 100,
                               activation = "Tanh")
```


```{r}
h2o.saveModel(model_nn_dim, path="model_nn_dim", force = TRUE)
```


```{r}
model_nn_dim <- h2o.loadModel("model_nn_dim/DeepLearning_model_R_1608771539567_1")
model_nn_dim
```


```{r}
dim(test)
test_dim <- h2o.deepfeatures(model_nn, test, layer = 3)
dim(test_dim)
```


```{r}
h2o.predict(model_nn_dim, test_dim) %>%
  as.data.frame() %>%
  mutate(actual = as.vector(test[, 31])) %>%
  group_by(actual, predict) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```
Now, this actually looks quite good in terms of identifying fraud cases: 92% of fraud cases were identified! However, many non-fraud cases were also classified as fraud. For real-life application, this wouldn’t be a good model. Let’s try some other techniques…

# Anomaly detection

We can also ask which instances were considered outliers or anomalies within our test data, using the h2o.anomaly() function. Based on the autoencoder model that was trained before, the input data will be reconstructed and for each instance, the mean squared error (MSE) between actual value and reconstruction is calculated.

I am also calculating the mean MSE for both class labels.
```{r}
anomaly <- h2o.anomaly(model_nn, test) %>%
  as.data.frame() %>%
  tibble::rownames_to_column() %>%
  mutate(Class = as.vector(test[, 31]))

mean_mse <- anomaly %>%
  group_by(Class) %>%
  summarise(mean = mean(Reconstruction.MSE))
```
```{r}
anomaly
```

This, we can now plot:
```{r}
ggplot(anomaly, aes(x = as.numeric(rowname), y = Reconstruction.MSE, color = as.factor(Class))) +
  geom_point(alpha = 0.3) +
  geom_hline(data = mean_mse, aes(yintercept = mean, color = Class)) +
  scale_color_brewer(palette = "Set1") +
  labs(x = "instance number",
       color = "Class")
```

As we can see in the plot, there is no perfect classification into fraud and non-fraud cases but the mean MSE is definitely higher for fraudulent transactions than for regular ones.

We can now identify outlier instances by applying an MSE threshold for what we consider outliers. We could e.g. say that we consider every instance with an MSE > 0.02 (chosen according to the plot above) to be an anomaly/outlier.
```{r}
anomaly <- anomaly %>%
  mutate(outlier = ifelse(Reconstruction.MSE > 0.02, "outlier", "no_outlier"))

anomaly %>%
  group_by(Class, outlier) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```
As we can see, outlier detection is not sufficient to correctly classify fraudulent credit card transactions either (at least not with this dataset).

# Pre-trained supervised model

We can now try using the autoencoder model as a pre-training input for a supervised model. Here, I am again using a neural network. This model will now use the weights from the autoencoder for model fitting.

```{r}
model_nn_2 <- h2o.deeplearning(y = response,
                               x = features,
                               training_frame = train_supervised,
                               pretrained_autoencoder  = "model_nn",
                               reproducible = TRUE, #slow - turn off for real problems
                               balance_classes = TRUE,
                               ignore_const_cols = FALSE,
                               seed = 42,
                               hidden = c(10, 2, 10), 
                               epochs = 100,
                               activation = "Tanh")
```


```{r}
h2o.saveModel(model_nn_2, path="model_nn_2", force = TRUE)
```


```{r}
model_nn_2 <- h2o.loadModel("model_nn_2/DeepLearning_model_R_1608771539567_22")
model_nn_2
```


```{r}
pred <- as.data.frame(h2o.predict(object = model_nn_2, newdata = test)) %>%
  mutate(actual = as.vector(test[, 31]))
```


```{r}
pred %>%
  group_by(actual, predict) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) 
```


```{r}
pred %>%
  ggplot(aes(x = actual, fill = predict)) +
    geom_bar() +
    theme_bw() +
    scale_fill_brewer(palette = "Set1") +
    facet_wrap( ~ actual, scales = "free", ncol = 2)
```

Now, this looks much better! We did miss 17% of the fraud cases but we also did not mis-classify too many of the non-fraud cases.

In real-life, we would now spend some more time trying to improve the model by e.g. performing grid search for hyperparameter tuning, going back to the original features (which we did not have here) and trying different engineered features and/or trying different algorithms. But here, I will leave it at that.

# Measuring model performance on highly unbalanced data

Because of the severe bias towards non-fraud cases, we can not use performance measures like accuracy or area under the curve (AUC), as they would give overly optimistic results based on the high percentage of correct classifications of the majority class.

An alternative to AUC is to use the precision-recall curve or the sensitivity (recall)-specificity curve. To calculate and plot these metrics, we can use the ROCR package. There are different ways to calculate the area under a curve (see the PRROC package for details) but I am going to use a simple function that calculates the area between every consecutive points-pair of x (i.e. x1 - x0, x2 - x1, etc.) under the corresponding values of y.
```{r}
library(ROCR)

# http://stackoverflow.com/questions/24563061/computing-integral-of-a-line-plot-in-r
line_integral <- function(x, y) {
  dx <- diff(x)
  end <- length(y)
  my <- (y[1:(end - 1)] + y[2:end]) / 2
  sum(dx * my)
} 

prediction_obj <- prediction(pred$p1, pred$actual)
```


```{r}
par(mfrow = c(1, 2))
par(mar = c(5.1,4.1,4.1,2.1))

# precision-recall curve
perf1 <- performance(prediction_obj, measure = "prec", x.measure = "rec") 

x <- perf1@x.values[[1]]
y <- perf1@y.values[[1]]
y[1] <- 0

plot(perf1, main = paste("Area Under the\nPrecision-Recall Curve:\n", round(abs(line_integral(x,y)), digits = 3)))

# sensitivity-specificity curve
perf2 <- performance(prediction_obj, measure = "sens", x.measure = "spec") 

x <- perf2@x.values[[1]]
y <- perf2@y.values[[1]]
y[1] <- 0

plot(perf2, main = paste("Area Under the\nSensitivity-Specificity Curve:\n", round(abs(line_integral(x,y)), digits = 3)))
```


```{r}
thresholds <- seq(from = 0, to = 1, by = 0.1)
pred_thresholds <- data.frame(actual = pred$actual)

for (threshold in thresholds) {
  
  prediction <- ifelse(pred$p1 > threshold, 1, 0)
  prediction_true <- ifelse(pred_thresholds$actual == prediction, TRUE, FALSE)
  pred_thresholds <- cbind(pred_thresholds, prediction_true)

}

colnames(pred_thresholds)[-1] <- thresholds
```


```{r}
pred_thresholds %>%
  gather(x, y, 2:ncol(pred_thresholds)) %>%
  group_by(actual, x, y) %>%
  summarise(n = n()) %>%
  ggplot(aes(x = as.numeric(x), y = n, color = actual)) +
    geom_vline(xintercept = 0.6, alpha = 0.5) +
    geom_line() +
    geom_point(alpha = 0.5) +
    theme_bw() +
    facet_wrap(actual ~ y, scales = "free", ncol = 2) +
    labs(x = "prediction threshold",
         y = "number of instances")
```


```{r}
pred %>%
  mutate(predict = ifelse(pred$p1 > 0.6, 1, 0)) %>%
  group_by(actual, predict) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) 
```


```{r}
sessionInfo()
```


```{r}
```

