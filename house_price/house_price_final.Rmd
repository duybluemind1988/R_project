---
title: "Untitled"
output: html_document
---
# 1. Get data
```{r}
library(tidyverse)# include dplyr, tidr, ggplot2, tibble, readr, purr
library(recipes)
library(data.table)
library(rsample)   # for data splitting
library(caret)     # for model packages
```


```{r}
path="house_price_train.csv"
data <- fread(path)
dim(data)
head(data)
```
#2. Preprocess data
## Remove column with high null value (>50%)
```{r}
null_value <- sapply(data, function(x) sum(is.na(x)))
class(null_value)
null_value_df <- as.data.frame(null_value) %>% arrange(desc(null_value)) %>% filter(null_value > 0)
# Name the unnamed first column
null_value_df <- cbind(Features= rownames(null_value_df), null_value_df)
rownames(null_value_df) <- NULL
# Add percentage column
total_rows <- dim(data)[1]
null_value_df <- null_value_df %>% mutate(percentage=null_value/total_rows*100)
null_value_df
```


```{r}
# Check which column have na value > 50 %
data[, which(colMeans(is.na(data)) > 0.5)]
# Check which column did not have na value > 50 %
col_low_na_value <- data[, which(colMeans(!is.na(data)) > 0.5)]
col_low_na_value
```

```{r}
data <- data %>% select(all_of(col_low_na_value))
dim(data)
```


```{r}
# Create training (80%) and test (20%) sets for the 
set.seed(430)
split = createDataPartition(data$SalePrice, p =0.8, list = FALSE)
train = data[split, ]
test = data[-split, ]
dim(train)
dim(test)
head(train)
```
```{r}
colSums(is.na(train))
```

## Try method not convert to factor
```{r}
library(parsnip)
```

```{r}
rf_model <- rand_forest(mtry = 10, trees = 200) %>%
      set_engine("ranger", importance = "impurity") %>%
      set_mode("regression") %>%
      fit(SalePrice ~ ., data = train)
rf_model
```
```{r}
# Khong ap dung duoc khi chua convert sang factor
glmn_fit <- 
  linear_reg(penalty = 0.001, mixture = 0.5) %>% 
  set_engine("glmnet") %>%
  fit(SalePrice ~ ., data = train)
glmn_fit
```
Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : contrasts can be applied only to factors with 2 or more levels
```{r}
# Khong ap dung duoc khi chua convert sang factor
xgboost_model <- boost_tree() %>% 
            set_engine("xgboost") %>% 
            set_mode("regression") %>% 
            fit(SalePrice ~ ., data = train)
xgboost_model
```
Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : contrasts can be applied only to factors with 2 or more levels
```{r}
# Khong ap dung duoc khi co qua nhieu NA
linear_fit <- 
        linear_reg() %>% 
        set_engine("lm") %>%
        set_mode("regression") %>% 
        fit(SalePrice ~ ., data = train)
```
Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : contrasts can be applied only to factors with 2 or more levels
```{r}
# Khong ap dung duoc khi co factor 1 level
linear_fit <- 
        linear_reg() %>% 
        set_engine("lm") %>%
        set_mode("regression") %>% 
        fit(SalePrice ~ ., data = drop_na(train))

```
Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : contrasts can be applied only to factors with 2 or more levels
```{r}
# convert training data to h2o object
response="SalePrice"
predictors <- setdiff(colnames(train), response)
train_h2o_new <- as.h2o(train)
tic()
h2o.gbm <- h2o.gbm(
  x = predictors, 
  y = response,
  training_frame = train_h2o_new, 
  #validation_frame = valid_h2o,
  balance_classes = TRUE,
  nfolds=5,
  #ntrees = 1000,
  #booster = "dart",
  #normalize_type = "tree",
  
  seed = 123
)
toc() # 7 s
test_pred <-h2o.predict(h2o.gbm, newdata = test_h2o)
h2o.gbm
```

There are three main steps in creating and applying feature engineering with recipes:
VERY IMPORTANT
- recipe: where you define your feature engineering steps to create your blueprint.
- prepare: estimate feature engineering parameters based on training data.
- bake: apply the blueprint to new data.

Receipe:
- Remove near-zero variance features that are categorical (aka nominal).
- Ordinal encode our quality-based features (which are inherently ordinal).
- Center and scale (i.e., standardize) all numeric features.
- Perform dimension reduction by applying PCA to all numeric features.

While your projectâ€™s needs may vary, here is a suggested order of potential steps that should work for most problems:

- Filter out zero or near-zero variance features.
- Perform imputation if required.
- Normalize to resolve numeric feature skewness.
- Standardize (center and scale) numeric features.
- Perform dimension reduction (e.g., PCA) on numeric features.
- One-hot or dummy encode categorical features.

Functions that specify the role of the variables:

- all_predictors() applies the step to the predictor variables only
- all_outcomes() applies the step to the outcome variable(s) only
Functions that specify the type of the variables:

- all_nominal() applies the step to all variables that are nominal (categorical)
- all_numeric() applies the step to all variables that are numeric

Predictor variables: are also known as independent variables, x-variables, and input variables. A predictor variable explains changes in the response. Typically, you want to determine how changes in one or more predictors are associated with changes in the response.

Outcome variables are usually the dependent variables which are observed and measured by changing independent variables. These variables determine the effect of the cause (independent) variables when changed for different values. The dependent variables are the outcomes of the experiments determining what was caused or what changed as a result of the study.

## Try method convert to factor
```{r}
library(tictoc)
library(h2o)
h2o.init()
```

```{r}
response="SalePrice"
recipe_obj <- recipe(SalePrice ~ ., data = train) %>%
  step_nzv(all_predictors())  %>% #Remove near-zero variance features 
  #step_knnimpute(all_predictors(), neighbors = 6) %>%  # Impute, very slow in large data, need step outside
  step_YeoJohnson(all_numeric(),-all_outcomes()) %>% # Remove skewness
  #step_integer(matches("Qual|Cond|QC|Qu")) %>% # Ordinal encoder
  step_center(all_numeric(), -all_outcomes()) %>% # center 
  step_scale(all_numeric(), -all_outcomes()) %>% # scale
  #step_dummy(all_nominal(), one_hot = TRUE) %>% 
  #step_pca(all_numeric(), -all_outcomes()) #Perform dimension reduction
  prep()
  
baked_train <- bake(recipe_obj, new_data = train)
#baked_valid <- bake(recipe_obj, new_data = valid)
baked_test <- bake(recipe_obj, new_data = test)

# convert training data to h2o object
train_h2o <- as.h2o(baked_train)
#valid_h2o <- as.h2o(baked_valid)
test_h2o <- as.h2o(baked_test)
# set the predictor names
predictors <- setdiff(colnames(baked_train), response)
```
```{r}
colSums(is.na(baked_train))
```

```{r}
# xgboost partsnip dung tot khi da convert sang factor 
xgboost_model <- boost_tree() %>% 
            set_engine("xgboost") %>% 
            set_mode("regression") %>% 
            fit(SalePrice ~ ., data = baked_train)
#xgboost_model
```

```{r}
# glmn_fit partsnip dung tot khi da convert sang factor , chap nhan NA value
glmn_fit <- 
  linear_reg(penalty = 0.001, mixture = 0.5) %>% 
  set_engine("glmnet") %>%
  set_mode("regression") %>% 
  fit(SalePrice ~ ., data = baked_train)
# glmn_fit
```

```{r}
# linear regression cannot work, may be NA value
linear_fit <- 
        linear_reg() %>% 
        set_engine("lm") %>%
        set_mode("regression") %>% 
        fit(SalePrice ~ ., data = baked_train)
```
Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : contrasts can be applied only to factors with 2 or more levels
```{r}
linear_fit <- 
        linear_reg() %>% 
        set_engine("lm") %>%
        set_mode("regression") %>% 
        fit(SalePrice ~ ., data = drop_na(baked_train))
```
Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : contrasts can be applied only to factors with 2 or more levels
```{r}
tic()
h2o.gbm <- h2o.gbm(
  x = predictors, 
  y = response,
  training_frame = train_h2o, 
  #validation_frame = valid_h2o,
  balance_classes = TRUE,
  nfolds=5,
  #ntrees = 1000,
  #booster = "dart",
  #normalize_type = "tree",
  
  seed = 123
)
toc() # 7 s
test_pred <-h2o.predict(h2o.gbm, newdata = test_h2o)
h2o.gbm
```

#3. Benchmark method
https://www.tidymodels.org/learn/models/parsnip-ranger-glmnet/

## 3.1 Data clean
```{r}
library(tidymodels)
library(data.table)
data(ames) # No na value, convert all to factor

#path="house_price_train.csv"
#ames <- fread(path)

set.seed(4595)
data_split <- initial_split(ames, strata = "Sale_Price", prop = 0.75)

ames_train <- training(data_split)
ames_test  <- testing(data_split)
ames_train
```
## RANDOM FOREST

```{r}
preds <- c("Longitude", "Latitude", "Lot_Area", "Neighborhood", "Year_Sold")

rf_xy_fit <- 
  rand_forest() %>%
  set_mode("regression") %>% 
  set_engine("ranger") %>%
  fit_xy(
    x = ames_train[, preds],
    y = log10(ames_train$Sale_Price)
  )

rf_xy_fit
```


```{r}
test_results <- 
  ames_test %>%
  select(Sale_Price) %>%
  mutate(Sale_Price = log10(Sale_Price)) %>%
  bind_cols(
    predict(rf_xy_fit, new_data = ames_test[, preds])
  )
test_results %>% slice(1:5)
```


```{r}
# summarize performance
test_results %>% metrics(truth = Sale_Price, estimate = .pred) 
```

mtry: The number of predictors that will be randomly sampled at each split when creating the tree models.
trees: The number of trees contained in the ensemble.

```{r}
rand_forest(mode = "regression", mtry = 3, trees = 1000) %>%
  set_engine("ranger") %>%
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,
    data = ames_train
  )
```
When the model it being fit by parsnip, data descriptors are made available. These attempt to let you know what you will have available when the model is fit. When a model object is created (say using rand_forest()), the values of the arguments that you give it are immediately evaluated unless you delay them. To delay the evaluation of any argument, you can used rlang::expr() to make an expression.


Two relevant data descriptors for our example model are:

.preds(): the number of predictor variables in the data set that are associated with the predictors prior to dummy variable creation.
.cols(): the number of predictor columns after dummy variables (or other encodings) are created.
```{r}
rand_forest(mode = "regression", mtry = .preds(), trees = 1000) %>%
  set_engine("ranger") %>%
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,
    data = ames_train
  )
```

## REGULARIZED REGRESSION
A linear model might work for this data set as well. We can use the linear_reg() parsnip model. There are two engines that can perform regularization/penalization, the glmnet and sparklyr packages. Letâ€™s use the former here. The glmnet package only implements a non-formula method, but parsnip will allow either one to be used.

When regularization is used, the predictors should first be centered and scaled before being passed to the model. The formula method wonâ€™t do that automatically so we will need to do this ourselves. Weâ€™ll use the recipes package for these steps.


```{r}
norm_recipe <- 
  recipe(
    Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, 
    data = ames_train
  ) %>%
  step_other(Neighborhood) %>% 
  step_dummy(all_nominal()) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_log(Sale_Price, base = 10) %>% 
  # estimate the means and standard deviations
  prep(training = ames_train, retain = TRUE)
norm_recipe
```
```{r}
baked_train<-bake(norm_recipe, new_data = ames_train)
baked_train
```


```{r}
# Now let's fit the model using the processed version of the data

glmn_fit <- 
  linear_reg(penalty = 0.001, mixture = 0.5) %>% 
  set_engine("glmnet") %>%
  fit(Sale_Price ~ ., data = baked_train)
glmn_fit
```


```{r}
# First, get the processed version of the test set predictors:
test_normalized <- bake(norm_recipe, new_data = ames_test, all_predictors())

test_results <- 
  test_results %>%
  rename(`random forest` = .pred) %>%
  bind_cols(
    predict(glmn_fit, new_data = test_normalized) %>%
      rename(glmnet = .pred)
  )
test_results
```
```{r}
test_results %>% metrics(truth = Sale_Price, estimate = glmnet) 
```
```{r}
test_results %>% 
  gather(model, prediction, -Sale_Price) %>% 
  ggplot(aes(x = prediction, y = Sale_Price)) + 
  geom_abline(col = "green", lty = 2) + 
  geom_point(alpha = .4) + 
  facet_wrap(~model) + 
  coord_fixed()
```

## 3.2 Data not clean
```{r}
library(tidymodels)
library(data.table)
#data(ames) # No na value, convert all to factor

path="house_price_train.csv"
ames <- fread(path)

set.seed(4595)
data_split <- initial_split(ames, strata = "SalePrice", prop = 0.75)

ames_train <- training(data_split)
ames_test  <- testing(data_split)
ames_train
```
```{r}
colSums(is.na(ames_train))
```

## Random forest
```{r}
rf_model <- rand_forest(mtry = 10, trees = 200) %>%
      set_engine("ranger", importance = "impurity") %>%
      set_mode("regression") %>%
      fit(SalePrice ~ ., data = ames_train)
rf_model

```

## REGULARIZED REGRESSION
```{r}
norm_recipe <- 
  recipe(
    SalePrice ~ ., 
    data = ames_train
  ) %>%
  step_rm(Id) %>% 
  step_nzv(all_predictors())  %>% #Remove near-zero variance features 
  #step_knnimpute(all_predictors(), neighbors = 6) %>%  # Impute, very slow in large data, need step outside
  step_other(Neighborhood) %>% 
  step_dummy(all_nominal()) %>%
  
  step_YeoJohnson(all_numeric(),-all_outcomes()) %>% # Remove skewness
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_log(SalePrice, base = 10) %>% 
  # estimate the means and standard deviations
  prep()
norm_recipe
```


```{r}
baked_train <- bake(norm_recipe, new_data = ames_train)
#baked_valid <- bake(recipe_obj, new_data = valid)
baked_test <- bake(norm_recipe, new_data = ames_test)
baked_train
```
```{r}
rf_model <- rand_forest(mtry = 10, trees = 200) %>%
      set_engine("ranger", importance = "impurity") %>%
      set_mode("regression") %>%
      fit(SalePrice ~ ., data = baked_train)
rf_model
```

```{r}
glmn_fit <- 
  linear_reg() %>% 
  set_engine("glmnet") %>%
  fit(SalePrice ~ ., data = baked_train)
glmn_fit
```
```{r}
xgboost_model <- boost_tree() %>% 
            set_engine("xgboost") %>% 
            set_mode("regression") %>% 
            fit(SalePrice ~ ., data = baked_train)
xgboost_model
```
```{r}
predict(xgboost_model,baked_test)
```


```{r}
```


```{r}
```

## H2O
```{r}
response="SalePrice"
# convert training data to h2o object
train_h2o <- as.h2o(baked_train)
#valid_h2o <- as.h2o(baked_valid)
test_h2o <- as.h2o(baked_test)
# set the predictor names
predictors <- setdiff(colnames(baked_train), response)
```


```{r}
tic()
h2o.gbm <- h2o.gbm(
  x = predictors, 
  y = response,
  training_frame = train_h2o, 
  #validation_frame = valid_h2o,
  #balance_classes = TRUE,
  #nfolds=5,
  #ntrees = 1000,
  #booster = "dart",
  #normalize_type = "tree",
  
  seed = 123
)
toc() # 7 s
test_pred <-h2o.predict(h2o.gbm, newdata = test_h2o)
h2o.gbm
```
```{r}
h2o.varimp_plot(h2o.gbm,num_of_features=20)
```


```{r}
h2o.shap_summary_plot(h2o.gbm,test_h2o)
```


```{r}
h2o.shap_explain_row_plot(h2o.gbm, test_h2o,row_index = 10)
h2o.shap_explain_row_plot(h2o.gbm, test_h2o,row_index = 4)
```


```{r}
```


```{r}
```


```{r}
```

