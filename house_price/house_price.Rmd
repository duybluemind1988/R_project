---
title: "House_price"
output: html_document
---
```{r}
#library(tidyverse)# include dplyr, tidr, ggplot2, tibble, readr, purr
library(dplyr) # mutate, select, filter, summarize, arrange, group by 
library(tidyr) # gather, spread, separate, extract, unite, %>%
library(ggplot2)
library(tibble) # provide tibble class (better than traditional data frame)
library(readr) # read_csv, tsv, delim, table, log....
library(purrr) # map, map_dbl, split,
library(data.table)
library(rsample)   # for data splitting
library(caret)     # for model packages
```

# Get and summary data
```{r}
path="house_price_train.csv"
data <- fread(path)
data
```

Here's a brief version of what you'll find in the data description file.

SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.
MSSubClass: The building class
MSZoning: The general zoning classification
LotFrontage: Linear feet of street connected to property
LotArea: Lot size in square feet
Street: Type of road access
Alley: Type of alley access
LotShape: General shape of property
LandContour: Flatness of the property
Utilities: Type of utilities available
LotConfig: Lot configuration
LandSlope: Slope of property
Neighborhood: Physical locations within Ames city limits
Condition1: Proximity to main road or railroad
Condition2: Proximity to main road or railroad (if a second is present)
BldgType: Type of dwelling
HouseStyle: Style of dwelling
OverallQual: Overall material and finish quality
OverallCond: Overall condition rating
YearBuilt: Original construction date
YearRemodAdd: Remodel date
RoofStyle: Type of roof
RoofMatl: Roof material
Exterior1st: Exterior covering on house
Exterior2nd: Exterior covering on house (if more than one material)
MasVnrType: Masonry veneer type
MasVnrArea: Masonry veneer area in square feet
ExterQual: Exterior material quality
ExterCond: Present condition of the material on the exterior
Foundation: Type of foundation
BsmtQual: Height of the basement
BsmtCond: General condition of the basement
BsmtExposure: Walkout or garden level basement walls
BsmtFinType1: Quality of basement finished area
BsmtFinSF1: Type 1 finished square feet
BsmtFinType2: Quality of second finished area (if present)
BsmtFinSF2: Type 2 finished square feet
BsmtUnfSF: Unfinished square feet of basement area
TotalBsmtSF: Total square feet of basement area
Heating: Type of heating
HeatingQC: Heating quality and condition
CentralAir: Central air conditioning
Electrical: Electrical system
1stFlrSF: First Floor square feet
2ndFlrSF: Second floor square feet
LowQualFinSF: Low quality finished square feet (all floors)
GrLivArea: Above grade (ground) living area square feet
BsmtFullBath: Basement full bathrooms
BsmtHalfBath: Basement half bathrooms
FullBath: Full bathrooms above grade
HalfBath: Half baths above grade
Bedroom: Number of bedrooms above basement level
Kitchen: Number of kitchens
KitchenQual: Kitchen quality
TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)
Functional: Home functionality rating
Fireplaces: Number of fireplaces
FireplaceQu: Fireplace quality
GarageType: Garage location
GarageYrBlt: Year garage was built
GarageFinish: Interior finish of the garage
GarageCars: Size of garage in car capacity
GarageArea: Size of garage in square feet
GarageQual: Garage quality
GarageCond: Garage condition
PavedDrive: Paved driveway
WoodDeckSF: Wood deck area in square feet
OpenPorchSF: Open porch area in square feet
EnclosedPorch: Enclosed porch area in square feet
3SsnPorch: Three season porch area in square feet
ScreenPorch: Screen porch area in square feet
PoolArea: Pool area in square feet
PoolQC: Pool quality
Fence: Fence quality
MiscFeature: Miscellaneous feature not covered in other categories
MiscVal: $Value of miscellaneous feature
MoSold: Month Sold
YrSold: Year Sold
SaleType: Type of sale
SaleCondition: Condition of sale
```{r}
# Using an insightful summary with skim and kable
data %>% glimpse()
```
```{r}
library(psych)
describe(data)
```

# Check null value

```{r}
dim(data)
sum(is.na(data))
```
```{r}
null_value <- sapply(data, function(x) sum(is.na(x)))
class(null_value)
null_value_df <- as.data.frame(null_value) %>% arrange(desc(null_value)) %>% filter(null_value > 0)
# Name the unnamed first column
null_value_df <- cbind(Features= rownames(null_value_df), null_value_df)
rownames(null_value_df) <- NULL
# Add percentage column
total_rows <- dim(data)[1]
null_value_df <- null_value_df %>% mutate(percentage=null_value/total_rows*100)
null_value_df
```

```{r}
library(Tmisc)
Tmisc::gg_na(data)
```
```{r}
#install.packages("VIM")
library(VIM)
VIM::aggr(data)
```


```{r}
aggr_plot <- VIM::aggr(data, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```

# Compute correlation matrix

```{r}

corr_matrix <- cor(data %>% select_if(is.numeric))
corr_matrix <- round(corr_matrix,2)
class(corr_matrix)
corr_matrix_df <- as.data.frame(apply(corr_matrix, 2, unlist)) %>% select(SalePrice) %>% arrange(desc(SalePrice))
# Name the unnamed first column
corr_matrix_df <- cbind(Features= rownames(corr_matrix_df), corr_matrix_df)
rownames(corr_matrix_df) <- NULL
corr_matrix_df
```
```{r}
# Top 10 for correlation
top_10 <-corr_matrix_df[1:11,1]
top_20 <-corr_matrix_df[1:21,1]
class(top_20)
top_20
```


# Correlation plot

```{r}
#install.packages('data.table', dependencies = TRUE)
#install.packages("DataExplorer", dependencies = TRUE)
library(DataExplorer)
DataExplorer::plot_correlation(data %>% select(top_20))
```
```{r}

```


```{r}
#install.packages("dlookr")
library(dlookr)
plot_correlate(data %>% select(top_20))
```

The function corrplot() takes the correlation matrix as the first argument. The second argument (type=“upper”) is used to display only the upper triangular of the correlation matrix.

```{r}
library(corrplot)
corrplot(corr_matrix, type = "upper", #order = "alphabet", 
         tl.col = "black", tl.srt = 45)
```
Positive correlations are displayed in blue and negative correlations in red color. Color intensity and the size of the circle are proportional to the correlation coefficients. In the right side of the correlogram, the legend color shows the correlation coefficients and the corresponding colors.

```{r}
corrplot.mixed(corr_matrix)
```


```{r}
library("PerformanceAnalytics")
chart.Correlation(data %>% select(top_10), histogram=TRUE, pch=19)
```

# DataExplorer package

```{r}
library(DataExplorer)
#DataExplorer::create_report(data) # Create full HTML report
create_report(data, y = "SalePrice")
```

```{r}
## View basic description for airquality data
introduce(data)
```

```{r}
## Plot basic description for airquality data
plot_intro(data)
```


```{r}
## View missing value distribution for airquality data
plot_missing(data)
```


```{r}
## Left: frequency distribution of all discrete variables
plot_bar(data)
```


```{r}
## Right: `price` distribution of all discrete variables
plot_bar(data, with = "SalePrice")
```
```{r}
head(data,10)
```

```{r}
## View frequency distribution by a discrete variable
plot_bar(data, by = "Condition1")
```

```{r}
## View histogram of all continuous variables
plot_histogram(data)
```
```{r}
## View estimated density distribution of all continuous variables
plot_density(data)
```


```{r}
## View quantile-quantile plot of all continuous variables
plot_qq(data)
```


```{r}
## View quantile-quantile plot of all continuous variables by feature `cut`
plot_qq(data, by = "BldgType")
```


```{r}
## View overall correlation heatmap
plot_correlation(data %>% select(top_20))
```


```{r}
## View bivariate continuous distribution based on `cut`
plot_boxplot(data,by="Utilities")
```


```{r}
## Scatterplot `price` with all other continuous features
plot_scatterplot(split_columns(data)$continuous, by = "SalePrice", sampled_rows = 1000L)
```
# Plotting PCA (Principal Component Analysis)
# Remove column which null value > 50%
```{r}
null_value_df
```
```{r}
null_col <- null_value_df %>% filter(percentage > 40) %>% select(Features)
null_col
```
```{r}
null_col$Features
```


```{r}
data_rm_null_col <- select(data,-null_col$Features)
dim(data)
dim(data_rm_null_col)
```
# Impute remaining null column (ERROR)


```{r}
```




# Remove Near Zero-Variance Predictors
```{r}
# Column value number have missing data
which(names(data_rm_null_col)%in%c(null_value_df$Features))
```

```{r}
nzv <- nearZeroVar(data_rm_null_col)
class(nzv)
nzv
data_rm_nzv <- select(data_rm_null_col,-nzv)
dim(data_rm_null_col)
dim(data_rm_nzv)
```

```{r}
null_value <- sapply(data_rm_nzv, function(x) sum(is.na(x)))
as.data.frame(null_value) %>% arrange(desc(null_value)) %>% filter(null_value > 0)
```
# Remove all NA row
```{r}
data_no_null <- na.omit(data_rm_nzv)
```

# Visualize principal component analysis (error)
```{r}
DataExplorer::plot_prcomp(data_no_null, maxcat = 5L)
```

```{r}
library(factoextra)
```

Visualize eigenvalues (scree plot). Show the percentage of variances explained by each principal component.
```{r}
res.pca <- prcomp(data_no_null %>% select_if(is.numeric), scale = TRUE)
fviz_eig(res.pca)
```
Graph of individuals. Individuals with a similar profile are grouped together.

```{r}
fviz_pca_ind(res.pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

Graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.
```{r}
fviz_pca_var(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

Biplot of individuals and variables
```{r}
fviz_pca_biplot(res.pca, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
                )
```


```{r}
# Eigenvalues
#eig.val <- get_eigenvalue(res.pca)
#eig.val
  
# Results for Variables
#res.var <- get_pca_var(res.pca)
#res.var$coord          # Coordinates
#res.var$contrib        # Contributions to the PCs
#res.var$cos2           # Quality of representation 
# Results for individuals
#res.ind <- get_pca_ind(res.pca)
#res.ind$coord          # Coordinates
#res.ind$contrib        # Contributions to the PCs
#res.ind$cos2           # Quality of representation
```

# Exploratory Data Analysis
https://cran.r-project.org/web/packages/dlookr/vignettes/EDA.html
```{r}
head(data)
```
# Test of normality on numeric variables using normality()

normality() performs a normality test on numerical data. Shapiro-Wilk normality test is performed. When the number of observations is greater than 5000, it is tested after extracting 5000 samples by random simple sampling.

The variables of tbl_df object returned by normality() are as follows.

- statistic : Statistics of the Shapiro-Wilk test
- p_value : p-value of the Shapiro-Wilk test
- sample : Number of sample observations performed Shapiro-Wilk test
normality() performs the normality test for all numerical variables of carseats as follows.:
```{r}
normality(data)
```

You can use dplyr to sort variables that do not follow a normal distribution in order of p_value:
```{r}
data %>%
  normality() %>%
  filter(p_value <= 0.01) %>% 
  arrange(abs(p_value))
```


```{r}
# Select columns by name
plot_normality(data, GrLivArea,GarageArea)
```
```{r}
plot_correlate(data,SalePrice)
```

# EDA when target variable is categorical variable
```{r}
head(data %>% select_if(is.character))
```

```{r}
categ <- target_by(data, Condition1)
categ
```


```{r}
# If the variable of interest is a numerical variable
cat_num <- relate(categ, SalePrice)
cat_num
```


```{r}
summary(cat_num)
```


```{r}
plot(cat_num)
```

Cases where predictors are categorical variable
```{r}
# If the variable of interest is a categorical variable
cat_cat <- relate(categ, HouseStyle)
cat_cat
```

```{r}
summary(cat_cat)
```


```{r}
plot(cat_cat)
```

# EDA when target variable is numerical variable
```{r}
head(data %>% select_if(is.numeric))
```


```{r}
# If the variable of interest is a numerical variable
num <- target_by(data, LotArea)
```


```{r}
# If the variable of interest is a numerical variable
num_num <- relate(num, SalePrice)
num_num
```

```{r}
summary(num_num)
```
plot() visualizes the relationship between the target and predictor variables. The relationship between Sales and Price is visualized with a scatter plot. The figure on the left shows the scatter plot of Sales and Price and the confidence interval of the regression line and regression line. The figure on the right shows the relationship between the original data and the predicted values of the linear model as a scatter plot. If there is a linear relationship between the two variables, the scatter plot of the observations converges on the red diagonal line.

```{r}
plot(num_num)
```


```{r}
plot(num_num, hex_thres = 350)
```

#Cases where predictors are categorical variable

The following example shows the relationship between ShelveLoc and the target variable Sales. The predictor ShelveLoc is a categorical variable and shows the result of one-way ANOVA of target ~ predictor relationship. The results are expressed in terms of ANOVA. The summary() function shows the regression coefficients for each level of the predictor. In other words, it shows detailed information about simple regression analysis of target ~ predictor relationship.
```{r}
# If the variable of interest is a categorical variable
num_cat <- relate(num, Condition2)
num_cat
```

# Creating an EDA report using eda_report() ERROR
```{r}
#install.packages("moments")
library(moments)
data_no_null %>%
  eda_report(target = SalePrice,output_format = "html")
```
#Chapter 4 Bivariate Graphs

# Grouped kernel density plots
```{r}
# plot the distribution of salaries 
# by rank using kernel density plots
ggplot(data, 
       aes(x = SalePrice, 
           fill = Street)) +
  geom_density(alpha = 0.4)
```
4.3.3 Box plots
```{r}
# plot the distribution of salaries by rank using boxplots
ggplot(data, 
       aes(x = HouseStyle, 
           y = SalePrice)) +
  geom_boxplot()
```
4.3.4 Violin plots
```{r}
# plot the distribution using violin and boxplots
ggplot(data, 
       aes(x = HouseStyle, 
           y = SalePrice)) +
  geom_violin(fill = "cornflowerblue") +
  geom_boxplot(width = .2, 
               fill = "orange",
               outlier.color = "orange",
               outlier.size = 2)  
```

4.3.5 Ridgeline plots
```{r}
library(ggridges)

ggplot(data, 
       aes(x = SalePrice, 
           y = HouseStyle, 
           fill = HouseStyle)) +
  geom_density_ridges() + 
  theme_ridges() +
  #labs("Highway mileage by auto class") +
  theme(legend.position = "none")
```

4.3.7.1 Combining jitter and boxplots
```{r}
# plot the distribution of salaries 
# by rank using jittering
library(scales)
ggplot(data, 
       aes(x = factor(HouseStyle), 
           y = SalePrice, 
           color = HouseStyle)) +
  geom_boxplot(size=1,
               outlier.shape = 1,
               outlier.color = "black",
               outlier.size  = 3) +
  geom_jitter(alpha = 0.5, 
              width=.2) + 
  scale_y_continuous() +
  #labs(title = "Academic Salary by Rank", 
  #     subtitle = "9-month salary for 2008-2009",
  #     x = "",
  #     y = "") +
  theme_minimal() +
  theme(legend.position = "none") +
  coord_flip()
```
# Chapter 5 Multivariate Graphs

5.1 Grouping
```{r}
# plot experience vs. salary (color represents rank)
ggplot(data, aes(x = LotArea, 
                     y = SalePrice, 
                     color=MSSubClass)) +
  geom_point()
```


```{r}
# plot experience vs. salary (color represents rank)
ggplot(data, aes(x = LotArea, 
                     y = SalePrice, 
                     color=LotShape,
                     size=YrSold)) +
  geom_point()
```
5.2 Faceting

```{r}
# plot salary histograms by rank
ggplot(data, aes(x = SalePrice)) +
  geom_histogram(fill = "cornflowerblue",
                 color = "white") +
  facet_wrap(~LotShape, ncol = 1) 
```


```{r}
ggplot(data, aes(x = SalePrice)) +
  geom_histogram(fill = "cornflowerblue",
                 color = "white") +
  facet_grid(HouseStyle ~ YrSold) 
```

#6. Machine learning
```{r}
#library(tidyverse)# include dplyr, tidr, ggplot2, tibble, readr, purr
library(dplyr) # mutate, select, filter, summarize, arrange, group by 
library(tidyr) # gather, spread, separate, extract, unite, %>%
library(ggplot2)
library(tibble) # provide tibble class (better than traditional data frame)
library(readr) # read_csv, tsv, delim, table, log....
library(purrr) # map, map_dbl, split,
library(data.table)
library(rsample)   # for data splitting
library(caret)     # for model packages
```

# Get and summary data
```{r}
path="house_price_train.csv"
data <- fread(path)
dim(data)
head(data)
```

# Remove column with high null value (>50%)
```{r}
null_value <- sapply(data, function(x) sum(is.na(x)))
class(null_value)
null_value_df <- as.data.frame(null_value) %>% arrange(desc(null_value)) %>% filter(null_value > 0)
# Name the unnamed first column
null_value_df <- cbind(Features= rownames(null_value_df), null_value_df)
rownames(null_value_df) <- NULL
# Add percentage column
total_rows <- dim(data)[1]
null_value_df <- null_value_df %>% mutate(percentage=null_value/total_rows*100)
null_value_df
```

```{r}
# Check which column have na value > 50 %
data[, which(colMeans(is.na(data)) > 0.5)]
```

```{r}
# Check which column did not have na value > 50 %
col_low_na_value <- data[, which(colMeans(!is.na(data)) > 0.5)]
col_low_na_value
```
```{r}
data <- data %>% select(all_of(col_low_na_value))
dim(data)
```

# Convert all character columns to factor:
```{r}
# all character columns to factor:
data <- mutate_if(data, is.character, as.factor)
```
```{r}
data
```

# Create training and test set

```{r}
# Create training (80%) and test (20%) sets for the 
set.seed(430)
split = createDataPartition(data$SalePrice, p =0.8, list = FALSE)
train = data[split, ]
test = data[-split, ]
dim(train)
dim(test)
```
There are three main steps in creating and applying feature engineering with recipes:
VERY IMPORTANT
- recipe: where you define your feature engineering steps to create your blueprint.
- prepare: estimate feature engineering parameters based on training data.
- bake: apply the blueprint to new data.

Receipe:
- Remove near-zero variance features that are categorical (aka nominal).
- Ordinal encode our quality-based features (which are inherently ordinal).
- Center and scale (i.e., standardize) all numeric features.
- Perform dimension reduction by applying PCA to all numeric features.

# Recipes
```{r}
library(tidyverse)
library(recipes)
```

```{r}
recipe(SalePrice ~ ., data = train) %>%
  step_nzv(all_nominal()) 
```
```{r}
log(1000)
log(100)
```

While your project’s needs may vary, here is a suggested order of potential steps that should work for most problems:

- Filter out zero or near-zero variance features.
- Perform imputation if required.
- Normalize to resolve numeric feature skewness.
- Standardize (center and scale) numeric features.
- Perform dimension reduction (e.g., PCA) on numeric features.
- One-hot or dummy encode categorical features.

Functions that specify the role of the variables:

- all_predictors() applies the step to the predictor variables only

- all_outcomes() applies the step to the outcome variable(s) only

Functions that specify the type of the variables:

- all_nominal() applies the step to all variables that are nominal (categorical)

- all_numeric() applies the step to all variables that are numeric

Predictor variables: are also known as independent variables, x-variables, and input variables. A predictor variable explains changes in the response. Typically, you want to determine how changes in one or more predictors are associated with changes in the response.

Outcome variables are usually the dependent variables which are observed and measured by changing independent variables. These variables determine the effect of the cause (independent) variables when changed for different values. The dependent variables are the outcomes of the experiments determining what was caused or what changed as a result of the study.
```{r}
blueprint <- recipe(SalePrice ~ ., data = train) %>%
  step_nzv(all_nominal())  %>% #Remove near-zero variance features 
  step_knnimpute(all_predictors(), neighbors = 6) %>%  # Impute, very slow in large data, need step outside
  step_YeoJohnson(all_numeric(),-all_outcomes()) %>% # Remove skewness
  step_integer(matches("Qual|Cond|QC|Qu")) %>% # Ordinal encoder
  step_center(all_numeric(), -all_outcomes()) %>% # center 
  step_scale(all_numeric(), -all_outcomes()) %>% # scale
  step_dummy(all_nominal(), one_hot = TRUE) %>% 
  step_pca(all_numeric(), -all_outcomes()) #Perform dimension reduction
blueprint
```
Prepare:

Next, we need to train this blueprint on some training data. Remember, there are many feature engineering steps that we do not want to train on the test data (e.g., standardize and PCA) as this would create data leakage. So in this step we estimate these parameters based on the training data of interest.
```{r}
prepare <- prep(blueprint, training = train)
prepare
```
Bake:

Lastly, we can apply our blueprint to new data (e.g., the training data or future test data) with bake()

```{r}
baked_train <- bake(prepare, new_data = train)
baked_test <- bake(prepare, new_data = test)
baked_train
```

```{r}
cv <- trainControl(
  method = "repeatedcv", 
  number = 5, #10
  repeats = 1 #5
)

knn_fit3 <- train(
  SalePrice~.,
  data = baked_train, 
  method = "knn", 
  trControl = cv, 
  #tuneGrid = hyper_grid,
  metric = "RMSE"
)
knn_fit3
```
```{r}
# plot cross validation results
ggplot(knn_fit3)
```

```{r}
pred <- predict(knn_fit3, baked_test)
RMSE(baked_test$SalePrice,pred) 
# RMSE in document is 19905
```

# Use recipe in train
Consequently, the goal is to develop our blueprint, then within each resample iteration we want to apply prep() and bake() to our resample training and validation data. Luckily, the caret package simplifies this process. We only need to specify the blueprint and caret will automatically prepare and bake within each resample. We illustrate with the ames housing example.


 Note: very slow due to preprocee data by blueprint each cv
 Specify resampling plan 
cv <- trainControl(
  method = "repeatedcv", 
  number = 5, #10
  repeats = 1 #5
)

Construct grid of hyperparameter values

hyper_grid <- expand.grid(k = seq(2, 8, by = 1))

Each parameter turning will have 5 cv for evaluation (repates 1 in each cv) 
Tune a knn model using grid search

knn_fit2 <- train(
  blueprint, 
  data = train, 
  method = "knn", 
  trControl = cv, 
  #tuneGrid = hyper_grid,
  metric = "RMSE"
)

```{r}
# print model results
#knn_fit2
```


```{r}
# plot cross validation results
#ggplot(knn_fit2)
```


```{r}
#pred <- predict(knn_fit2, test)
#RMSE(test$SalePrice,pred) 
```
# Check all method
# Do Parallel
```{r}
library(doParallel)
cl <-makePSOCKcluster(5)
registerDoParallel(cl)
```
```{r}
library(caret) # logistic regression, lm, knn, 
library(glmnet)   # for implementing regularized regression
library(earth)     # for fitting MARS models
library(rpart) # decision tree
library(ipred)       # for fitting bagged decision trees
library(ranger)   # a c++ implementation of random forest 
```


```{r}
cv <- trainControl(
  method = "cv", 
  number = 4, #10
)
# Create blank modelList
modelList <- vector(mode = "list", length = 0)
timeList <- vector(mode = "list", length = 0)

list_model <- c("knn","glmnet","svmRadial","cubist","lm","pcr","pls","bagEarth","ranger","svmLinear") 
# knn,glmnet,svmRadial,lm,pcr,pls: 1 s
# cubist: 3 s
# xgbTree,xgblinear : very slow  # 2,7 min
# ranger: c++ implementation of random forest  # 10 s
# glmnet: lasso and ridge
# bagEarth: Bagged  Multivariate adaptive regression splines # 7s
# pcr: Principal component regression
# pls: Partial least squares
# xgbLinear: eXtreme Gradient Boosting
for (model_name in list_model){
  print(model_name)
  set.seed(7)
  start_time <- lubridate::minute(Sys.time())
  
  Model <- train(SalePrice~., data=baked_train, method=model_name, metric="RMSE", trControl=cv)
  
  end_time <- lubridate::minute(Sys.time())
  time <- (end_time - start_time)
  print(time)
  
  modelList[[model_name]] <- Model
  timeList[[model_name]] <- time
}

```

```{r}
#bwplot(resamples(modelList),metric="RMSE")
bwplot(resamples(modelList))
bwplot(resamples(modelList),metric="Rsquared")
```
```{r}
time_s <- t(data.frame(timeList)) # transpose
time_df<-data.frame(time_s)
# name id column in R
time_df <- cbind(model_name= rownames(time_df), time= time_df)
rownames(time_df) <- NULL
ggplot(time_df,aes(model_name,time_s))+geom_point()+ geom_line(aes(group = 1))
```

```{r}
summary(resamples(modelList))
```

```{r}
modelList[["knn"]]$results
```
Prediction:
```{r}
for (model_name in names(modelList) ){
  print(model_name)
  pred <- predict(modelList[[model_name]], baked_test)
  print(RMSE(baked_test$SalePrice,pred) )
}
```
No different between select target vs not select target in predict
for (model_name in names(modelList) ){
  print(model_name)
  pred <- predict(modelList[[model_name]], baked_test %>% select(-SalePrice))
  print(RMSE(baked_test$SalePrice,pred) )
}
# Feature important

Best model: ranger (C++ rf), cubist, bagEarth
```{r}
modelList["knn"]$knn
modelList["knn"]$knn$bestTune
modelList["ranger"]$ranger
modelList["ranger"]$ranger$bestTune
modelList["pls"]$pls
modelList["pls"]$pls$bestTune
```
```{r}
names(modelList)
```

```{r}
vip::vip(modelList["pls"]$pls, num_features = 20, method = "model")
vip::vip(modelList["glmnet"]$glmnet, num_features = 20, method = "model")
vip::vip(modelList["bagEarth"]$bagEarth, num_features = 20, method = "model")
vip::vip(modelList["cubist"]$cubist, num_features = 20, method = "model")
```
```{r}
library(pdp)
pdp::partial(modelList["cubist"]$cubist, "PC2", grid.resolution = 20, plot = TRUE)
pdp::partial(modelList["cubist"]$cubist, "PC1", grid.resolution = 20, plot = TRUE)
```
Important feature ranger
```{r}
cv <- trainControl(
  method = "cv", 
  number = 4, #10
)

fit <- train(
    SalePrice~.,
    data = baked_train,
    method = "ranger",
    trControl = trainControl(method="cv", number = 4, allowParallel = TRUE, verbose = TRUE),
    importance = 'impurity'
)
```

```{r}
fit
```
```{r}
varImp(fit)
```


# H2O model
Prepare data for H2O model
```{r}
path="house_price_train.csv"
data <- fread(path)
dim(data)
#head(data)
# Remove column with high null value (>50%)
null_value <- sapply(data, function(x) sum(is.na(x)))
class(null_value)
null_value_df <- as.data.frame(null_value) %>% arrange(desc(null_value)) %>% filter(null_value > 0)
# Name the unnamed first column
null_value_df <- cbind(Features= rownames(null_value_df), null_value_df)
rownames(null_value_df) <- NULL
# Add percentage column
total_rows <- dim(data)[1]
null_value_df <- null_value_df %>% mutate(percentage=null_value/total_rows*100)
null_value_df
# Check which column did not have na value > 50 %
col_low_na_value <- data[, which(colMeans(!is.na(data)) > 0.5)]
col_low_na_value
data <- data %>% select(all_of(col_low_na_value))
dim(data)
# all character columns to factor:
data <- mutate_if(data, is.character, as.factor)
# Create training (80%) and test (20%) sets for the 
set.seed(430)
split = createDataPartition(data$SalePrice, p =0.8, list = FALSE)
train = data[split, ]
test = data[-split, ]
dim(train)
dim(test)
```
```{r}
library(recipes)
blueprint <- recipe(SalePrice ~ ., data = train) %>%
  step_nzv(all_nominal())  %>% #Remove near-zero variance features 
  step_knnimpute(all_predictors(), neighbors = 6) %>%  # Impute, very slow in large data, need step outside
  step_YeoJohnson(all_numeric(),-all_outcomes()) %>% # Remove skewness
  step_integer(matches("Qual|Cond|QC|Qu")) %>% # Ordinal encoder
  step_center(all_numeric(), -all_outcomes()) %>% # center 
  step_scale(all_numeric(), -all_outcomes()) %>% # scale
  step_dummy(all_nominal(), one_hot = TRUE) %>% 
  step_pca(all_numeric(), -all_outcomes()) #Perform dimension reduction
prepare <- prep(blueprint, training = train)
baked_train <- bake(prepare, new_data = train)
baked_test <- bake(prepare, new_data = test)
```

```{r}
library(h2o)
h2o.no_progress()
h2o.init(max_mem_size = "5g")
```

```{r}
# convert training data to h2o object
train_h2o <- as.h2o(baked_train)
test_h2o <- as.h2o(baked_test)
# set the response column to Sale_Price
response <- "SalePrice"
n_features <- length(setdiff(names(baked_train), "SalePrice"))
# set the predictor names
predictors <- setdiff(colnames(baked_train), response)
```

Random Forest

```{r}
start_time <- lubridate::second(Sys.time())
h2o_rf1 <- h2o.randomForest(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    ntrees = n_features * 10, # very slow if high dim, not pca
    #ntrees=300,
    seed = 123
)
end_time <- lubridate::second(Sys.time())
h2o_rf1
pred <-h2o.predict(h2o_rf1, newdata = test_h2o)
print(RMSE(test_h2o$SalePrice,pred) )
time <- (end_time - start_time)
print(time)
# from 10 s to 1.6 s with RF
```

```{r}
p1 <- vip::vip(h2o_rf1, num_features = 25, bar = TRUE)
p1
```

```{r}
# No different between slect or not select target in prediction
max_col_num <- dim(test_h2o)[2]
pred <-h2o.predict(h2o_rf1, newdata = test_h2o[,2:max_col_num])
print(RMSE(test_h2o$SalePrice,pred) )
```

Gradient Boosting Machine (GBM)

```{r}
start_time <- lubridate::second(Sys.time())
h2o_gbm <- h2o.gbm(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    ntrees = n_features * 10, # very slow if high dim, not pca
    learn_rate=0.2,
    seed = 123
)
end_time <- lubridate::second(Sys.time())
h2o_gbm
pred <-h2o.predict(h2o_gbm, newdata = test_h2o)
print(RMSE(test_h2o$SalePrice,pred) )
time <- (end_time - start_time)
print(time)
```
```{r}
# To obtain the Mean-squared Error by tree from the model object:
h2o_gbm@model$scoring_history
```

Generalized Linear Models (GLM)

```{r}
start_time <- lubridate::second(Sys.time())
h2o_glm <- h2o.glm(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    #lambda_search = TRUE,
    seed = 123
)
end_time <- lubridate::second(Sys.time())
h2o_glm
pred <-h2o.predict(h2o_glm, newdata = test_h2o)
print(RMSE(test_h2o$SalePrice,pred) )
time <- (end_time - start_time)
print(time)
```
Deep learning

```{r}
start_time <- lubridate::second(Sys.time())
h2o_dl <- h2o.deeplearning(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    hidden = 25,
    seed = 123
)
end_time <- lubridate::second(Sys.time())
h2o_dl
pred <-h2o.predict(h2o_dl, newdata = test_h2o)
print(RMSE(test_h2o$SalePrice,pred) )
time <- (end_time - start_time)
print(time)
```


```{r}
h2o.performance(h2o_dl,test_h2o)
```

XGBoost

```{r}
start_time <- lubridate::second(Sys.time())
h2o_model <- h2o.xgboost(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    seed = 123
)
end_time <- lubridate::second(Sys.time())
h2o_model
pred <-h2o.predict(h2o_model, newdata = test_h2o)
print(RMSE(test_h2o$SalePrice,pred) )
time <- (end_time - start_time)
print(time)
# From 2.7 minutes to 1.69 s for xgboost
```

```{r}
# Eval performance:
perf <- h2o.performance(h2o_model)
perf
# Generate predictions on a test set (if necessary):
pred <- h2o.predict(h2o_model, newdata = test_h2o)

# Extract feature interactions:
#feature_interactions = h2o.feature_interaction(h2o_model)
#feature_interactions
```

#  Stacked Models H2O (GLM, RF, GBM, XGB)

```{r}
#library(tidyverse)# include dplyr, tidr, ggplot2, tibble, readr, purr
library(dplyr) # mutate, select, filter, summarize, arrange, group by 
library(tidyr) # gather, spread, separate, extract, unite, %>%
library(ggplot2)
library(tibble) # provide tibble class (better than traditional data frame)
library(readr) # read_csv, tsv, delim, table, log....
library(purrr) # map, map_dbl, split,
library(data.table)
library(rsample)   # for data splitting
library(caret)     # for model packages
library(recipes)
library(h2o)
h2o.no_progress()
h2o.init(max_mem_size = "2g")
```

```{r}
path="house_price_train.csv"
data <- fread(path)
dim(data)
#head(data)
# Remove column with high null value (>50%)
null_value <- sapply(data, function(x) sum(is.na(x)))
class(null_value)
null_value_df <- as.data.frame(null_value) %>% arrange(desc(null_value)) %>% filter(null_value > 0)
# Name the unnamed first column
null_value_df <- cbind(Features= rownames(null_value_df), null_value_df)
rownames(null_value_df) <- NULL
# Add percentage column
total_rows <- dim(data)[1]
null_value_df <- null_value_df %>% mutate(percentage=null_value/total_rows*100)
null_value_df
# Check which column did not have na value > 50 %
col_low_na_value <- data[, which(colMeans(!is.na(data)) > 0.5)]
col_low_na_value
data <- data %>% select(all_of(col_low_na_value))
dim(data)
# all character columns to factor:
data <- mutate_if(data, is.character, as.factor)
# Create training (80%) and test (20%) sets for the 
set.seed(430)
split = createDataPartition(data$SalePrice, p =0.8, list = FALSE)
train = data[split, ]
test = data[-split, ]
dim(train)
dim(test)

```


```{r}
blueprint <- recipe(SalePrice ~ ., data = train) %>%
  step_nzv(all_nominal())  %>% #Remove near-zero variance features 
  step_knnimpute(all_predictors(), neighbors = 6) %>%  # Impute, very slow in large data in train, need step outside
  step_YeoJohnson(all_numeric(),-all_outcomes()) %>% # Remove skewness
  step_integer(matches("Qual|Cond|QC|Qu")) %>% # Ordinal encoder
  step_center(all_numeric(), -all_outcomes()) %>% # center 
  step_scale(all_numeric(), -all_outcomes()) #%>% # scale
  #step_dummy(all_nominal(), one_hot = TRUE) %>% 
  #step_pca(all_numeric(), -all_outcomes()) #Perform dimension reduction
```


```{r}
# Create training & test sets for h2o
train_h2o <- prep(blueprint, training = train, retain = TRUE) %>%
  juice() %>%
  as.h2o()
test_h2o <- prep(blueprint, training = train) %>%
  bake(new_data = test) %>%
  as.h2o()

# Get response and feature names
Y <- "SalePrice"
X <- setdiff(names(train_h2o), Y)
```

```{r}
train_h2o
```

```{r}
# Train & cross-validate a GLM model
best_glm <- h2o.glm(
  x = X, y = Y, training_frame = train_h2o, alpha = 0.1,
  remove_collinear_columns = TRUE, nfolds = 10, fold_assignment = "Modulo",
  keep_cross_validation_predictions = TRUE, seed = 123
)

# Train & cross-validate a RF model
best_rf <- h2o.randomForest(
  x = X, y = Y, training_frame = train_h2o, ntrees = 300, #mtries = 20,
  max_depth = 30, min_rows = 1, sample_rate = 0.8, nfolds = 10,
  fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,
  seed = 123, stopping_rounds = 50, stopping_metric = "RMSE",
  stopping_tolerance = 0
)

# Train & cross-validate a GBM model
best_gbm <- h2o.gbm(
  x = X, y = Y, training_frame = train_h2o, ntrees = 300, learn_rate = 0.01,
  max_depth = 7, min_rows = 5, sample_rate = 0.8, nfolds = 10,
  fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,
  seed = 123, stopping_rounds = 50, stopping_metric = "RMSE",
  stopping_tolerance = 0
)

# Train & cross-validate an XGBoost model
best_xgb <- h2o.xgboost(
  x = X, y = Y, training_frame = train_h2o, ntrees = 300, learn_rate = 0.05,
  max_depth = 3, min_rows = 3, sample_rate = 0.8, #categorical_encoding = "Enum",
  nfolds = 10, fold_assignment = "Modulo", 
  keep_cross_validation_predictions = TRUE, seed = 123, stopping_rounds = 50,
  stopping_metric = "RMSE", stopping_tolerance = 0
)
```

```{r}
best_glm
best_rf
best_gbm
best_xgb
```
Model with data one hot encoder and PCA

H2ORegressionMetrics: glm
** Reported on cross-validation data. **
** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) **

MSE:  1549831955
RMSE:  39367.91
MAE:  25006.05
RMSLE:  NaN
Mean Residual Deviance :  1549831955
R^2 :  0.7574324
Null Deviance :7.483396e+12
Null D.o.F. :1168
Residual Deviance :1.811754e+12
Residual D.o.F. :1163
AIC :28069.17

H2ORegressionMetrics: drf
** Reported on cross-validation data. **
** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) **

MSE:  1137658043
RMSE:  33729.19
MAE:  20646.21
RMSLE:  0.1679699
Mean Residual Deviance :  1137658043

H2ORegressionMetrics: gbm
** Reported on cross-validation data. **
** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) **

MSE:  1147789389
RMSE:  33879.04
MAE:  21544.07
RMSLE:  0.1697947
Mean Residual Deviance :  1147789389

H2ORegressionMetrics: xgboost
** Reported on cross-validation data. **
** 10-fold cross-validation on training data (Metrics computed for combined holdout predictions) **

MSE:  1196728797
RMSE:  34593.77
MAE:  21019.3
RMSLE:  0.1681697
Mean Residual Deviance :  1196728797
```{r}
# Train a stacked tree ensemble
ensemble_tree <- h2o.stackedEnsemble(
  x = X, y = Y, training_frame = train_h2o, model_id = "my_tree_ensemble",
  base_models = list(best_glm, best_rf, best_gbm, best_xgb),
  metalearner_algorithm = "drf"
)
```


```{r}
# Get results from base learners
get_rmse <- function(model) {
  results <- h2o.performance(model, newdata = test_h2o)
  results@metrics$RMSE
}
list(best_glm, best_rf, best_gbm, best_xgb) %>%
  purrr::map_dbl(get_rmse)
## [1] 30024.67 23075.24 20859.92 21391.20

# Stacked results
h2o.performance(ensemble_tree, newdata = test_h2o)@metrics$RMSE
# data with one hot and pca: 28175.82
# data with no one hot and pca: 20236.79 # VERY BEST
## Result from ML book [1] 20664.56
```
```{r}
h2o.performance(best_glm, newdata = test_h2o)@metrics$RMSE # Worse RMSE
h2o.performance(best_rf, newdata = test_h2o)@metrics$RMSE
h2o.performance(best_gbm, newdata = test_h2o)@metrics$RMSE # BEST RMSE, near the same as esemble_tree
h2o.performance(best_xgb, newdata = test_h2o)@metrics$RMSE

```


```{r}
data.frame(
  GLM_pred = as.vector(h2o.getFrame(best_glm@model$cross_validation_holdout_predictions_frame_id$name)),
  RF_pred = as.vector(h2o.getFrame(best_rf@model$cross_validation_holdout_predictions_frame_id$name)),
  GBM_pred = as.vector(h2o.getFrame(best_gbm@model$cross_validation_holdout_predictions_frame_id$name)),
  XGB_pred = as.vector(h2o.getFrame(best_xgb@model$cross_validation_holdout_predictions_frame_id$name))
) %>% cor()
```

# Auto ML H2O
```{r}
# Use AutoML to find a list of candidate models (i.e., leaderboard)
auto_ml <- h2o.automl(
  x = X, y = Y, training_frame = train_h2o, nfolds = 4, 
  max_runtime_secs = 60 * 20, max_models = 50,
  keep_cross_validation_predictions = TRUE, sort_metric = "RMSE", seed = 123,
  stopping_rounds = 50, stopping_metric = "RMSE", #stopping_tolerance = 0
)
```


```{r}
# Assess the leader board; the following truncates the results to show the top 
# and bottom 15 models. You can get the top model with auto_ml@leader
auto_ml@leaderboard %>% 
  as.data.frame() %>%
  dplyr::select(model_id, rmse) %>%
  dplyr::slice(1:25)
```

```{r}
h2o.ls()
h2o.removeAll()
h2o.ls()
```

# Compare ML method handle missing data with R
```{r}
library(tidyverse)
library(caret)
library(data.table)
library(recipes)
```

```{r}
path_train="house_price_train.csv"
path_test="house_price_test.csv"
train_data <- fread(path_train)
test_data <- fread(path_test)
dim(train_data)
dim(test_data)
```
```{r}
library(VIM)
VIM::aggr(train_data)
```
```{r}
train_dataNoNA <- na.exclude(train_data)
VIM::aggr(train_dataNoNA)
```

# 1. Use recipes


```{r}
train_data <- mutate_if(train_data, is.character, as.factor)
test_data <- mutate_if(test_data, is.character, as.factor)
```
```{r}
head(train_data)
```

```{r}
library(Hmisc)
impute(train_data$Alley, "other")
impute(train_data, "other")
```

```{r}
blueprint <- recipe(SalePrice ~ ., data = train_data) %>%
  step_nzv(all_nominal())  %>% #Remove near-zero variance features 
  #step_knnimpute(all_predictors(), neighbors = 6) %>%  # Impute
  step_YeoJohnson(all_numeric(),-all_outcomes()) %>% # Remove skewness
  #step_integer(matches("Qual|Cond|QC|Qu")) %>% # Ordinal encoder
  step_center(all_numeric(), -all_outcomes()) %>% # center 
  step_scale(all_numeric(), -all_outcomes()) # scale
prepare <- prep(blueprint, training = train_data)
prepare
```
```{r}
baked_train <- bake(prepare, new_data = train_data)
baked_test <- bake(prepare, new_data = test_data)
#head(baked_train)
```

```{r}
head(baked_train)
```

```{r}
cv_glmnet <- train(
  SalePrice ~.,
  data=baked_train,
  method = "ranger",
  trControl = trainControl(method = "cv", number = 4),
)
```

# 2. Use caret preprocess
```{r}
library(mice)
complete(mice(data = train_data, m = 1, method = "mean"))
```

```{r}
cv_glmnet <- train(
  SalePrice ~.,
  data=train_data,
  method = "ranger",
  preProc = c("nzv","YeoJohnson","center", "scale"),
  trControl = trainControl(method = "cv", number = 4),
)
```

# Method from kaggle (OK)
https://www.kaggle.com/theslwayne/housing-data

NOTE:

uu diem cua phuong phap nay: khong can loai bo missing value, tuy nhien phai label encoding toan bo factor, dieu nay co the gay hieu nham trong thuat toan. Co the Impute cac missing value thanh other label, sau do one hot encoding va reduce pca
```{r}
# Indecies for splitting the data for testing and training
inTrain <- createDataPartition(train_data$SalePrice, p = 0.8, list = F, times = 1)
dim(inTrain)
```
```{r}
head(train_data)
```


```{r}
# Converting dataset into a matrix
training_data_matrix <- train_data %>%
    select(-SalePrice) %>% # Removing target values from the matrix
    mutate_if(is.character, as.factor) %>% # Converting String variables into factors
    mutate_if(is.factor, as.numeric) %>% # Label encoding
    #select_if(is.numeric) %>%
    as.matrix()

```

```{r}
dim(train_data)
dim(training_data_matrix)
head(training_data_matrix)
```


```{r}
# Train a XGBoost model
library(xgboost)
model_xgb1 <- xgboost(data = training_data_matrix[inTrain, ],
                      label = train_data$SalePrice[inTrain], 
                      nrounds = 50, eval_metric = "rmse",verbose=0)
```


```{r}
# Predicting using the test data
testPreds <- predict(model_xgb1, training_data_matrix[-inTrain, ])

# Print the mean error
RMSE(train_data$SalePrice[-inTrain],testPreds)
```

Making Predictions
```{r}
# Preprocessing and making a matrix from the dataset
test_data_matrix <- test_data %>%
    mutate_if(is.character, as.factor) %>%
    mutate_if(is.factor, as.numeric) %>%
    #select_if(is.numeric) %>%
    as.matrix()
```

```{r}
# Predicting Prices
subPreds <- predict(model_xgb1, test_data_matrix)
# Format CSV as specified in the competition discription
predFile <- tibble(Id = test_data$Id, SalePrice = subPreds)
# Saving file
write_csv(predFile, "submission.csv")
```


```{r}
```


```{r}
```


```{r}
```

