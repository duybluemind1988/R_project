---
title: "Untitled"
output: html_document
---
# Method 1
https://www.kaggle.com/gustavomodelli/bike-model-xgboost-h2o

```{r}
library(tidyverse) # metapackage with lots of helpful functions
library(lubridate)
library(caret)
library(recipes)
library(h2o)
```


```{r}
bike =read.csv("bike_train.csv")
bike
```


```{r}
str(bike)
```


```{r}
## Check missing data
bike  %>% map(~ sum(is.na(.)))
```


```{r}
## Feature engenearing
bike$datetime <- as_datetime(bike$datetime)
bike$day_week <- wday(bike$datetime, label = TRUE)
bike$month <- lubridate::month(bike$datetime, label = TRUE)
bike$hour <- lubridate::hour(bike$datetime)
bike
```
- instant: record index
- dteday : date
- season : season (1:winter, 2:spring, 3:summer, 4:fall)
- yr : year (0: 2011, 1:2012)
- mnth : month ( 1 to 12)
- hr : hour (0 to 23)
- holiday : weather day is holiday or not (extracted from [Web Link])
- weekday : day of the week
- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.
+ weathersit :
- 1: Clear, Few clouds, Partly cloudy, Partly cloudy
- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
- temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)
- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)
- hum: Normalized humidity. The values are divided to 100 (max)
- windspeed: Normalized wind speed. The values are divided to 67 (max)
- casual: count of casual users
- registered: count of registered users
- cnt: count of total rental bikes including both casual and registered
```{r}
## Converting Factors
bike <- bike %>% 
  mutate(
    holiday = factor(holiday, labels = c('no','yes')),
    workingday = factor(workingday, labels = c('no', 'yes')),
    season = factor(season, labels = c('spring','summer','fall','winter')),
    day_hour = ifelse(hour > 6 & hour < 10, 'between_6_and_10',
                      ifelse(hour > 10 & hour < 17, 'between_10_and_17',
                             ifelse(hour > 17 & hour < 20,
                                    'between_17_and_20', 'between_20_and_6'))),
    day_hour = factor(day_hour, levels = c('between_6_and_10','between_10_and_17','between_17_and_20','between_20_and_6')),
    weather = factor(weather, labels = c('Clear','Cloudy', 'Light_Rain','Heavy_Hain'))
  )

bike
```

Distribuiton of Target
```{r}
bike.sel <- bike  %>% filter(!is.na(count))

ggplot(bike.sel, aes(count))+
  geom_histogram(fill = 'blue')
```
 Bike Rent by Hour
```{r}

ggplot(bike.sel, aes(day_hour, count, fill = day_hour))+
  geom_boxplot()+
  coord_flip()+
  labs(title = 'Bike Rent by day Hour', x = '', y = '')+
  theme(legend.position = 'none')+
  theme_classic()
```
Bike rent by hour and seasson
```{r}

ggplot(bike.sel, aes(as.factor(hour), count, fill = season))+
  geom_boxplot()+
  facet_wrap(~ season)+
  theme_classic()
```

Bike rental by month
```{r}

ggplot(bike.sel, aes(month, count, fill = season))+
  geom_boxplot()+
  theme_classic()
```

Bike rental by Day week
```{r}
ggplot(bike.sel, aes(day_week, count, fill = day_week))+
  geom_boxplot()+
  theme(legend.position = 'none')+
  theme_classic()+
  coord_flip()
```
Bike rental by Weather

```{r}
ggplot(bike.sel, aes(weather, count, fill = weather))+
geom_boxplot()
```
```{r}
bike
```


```{r}
## -- Recipe --------------------------- ##
rec <- recipe(count ~ ., data = bike) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes(), -hour) %>% 
  step_center(all_numeric(), - all_outcomes(), -hour) %>% 
  step_scale(all_numeric(), -all_outcomes(), -hour) %>% 
  step_ns(hour, deg_free = 6) %>% 
  step_dummy(all_nominal()) %>% 
  step_mutate(count = log ( count + 1))
preparo <- prep(rec, bike)
bike.new.full <- bake(preparo, bike)
bike.new.full
```
Explain: 
- step_YeoJohnson(all_numeric(), -all_outcomes(), -hour): The Yeo-Johnson transformation is very similar to the Box-Cox but does not require the input variables to be strictly positive. Các giá trị cách xa nhau vd từ 0-32 sẽ gần lại từ 0-6 chẳng hạn (registered)

- step_ns(hour, deg_free = 6): can create new features from a single variable that enable fitting routines to model this variable in a nonlinear manner. Trong ví dụ này tạo ra 6 column hour (có thể explain cho 24 hours). Không tạo ra 24 feature mới mà chỉ tạo ra 6 thôi
- step_dummy(all_nominal()): one hot encoder toàn bộ factor
- step_mutate(count = log ( count + 1)) : convert count to log

Model with H2o
```{r}
##h2o init

h2o.init()
#sum(is.na(bike.new.full))
```
Must convert datetime Posxixt to date_time before using as.H2O
```{r}
bike.new.full$datetime <-as.Date(bike.new.full$datetime)
#bike.new.full
```

```{r}
## Load data to h2o
bike.h2o <- as.h2o(bike.new.full)

## Split data

split <- h2o.splitFrame(bike.h2o, ratios = 0.75)
bike.train <- split[[1]]
bike.test <- split[[2]]


## Variables

x <- setdiff(names(bike.h2o), 'count')
y <- 'count'
```
```{r}
bike.train
```

```{r}
bike.rf <- h2o.randomForest(x = x, y = y, training_frame = bike.train,
                            #nfolds = 5
                            )
h2o.performance(bike.rf, newdata = bike.test)
```

```{r}
bike.gbm <- h2o.gbm(x = x, y = y, training_frame = bike.train,
 #                           nfolds = 5
)
h2o.performance(bike.gbm, newdata = bike.test)
```

```{r}
h2o.varimp_plot(bike.rf)
h2o.varimp_plot(bike.gbm)
```
# Method 2
https://www.kaggle.com/roshanchoudhary/bike-sharing-in-r-programming

```{r}
library(tidyverse) # metapackage with lots of helpful functions
library(lubridate)
```


```{r}
bike_share_train <- read.csv("bike_train.csv", header=T)
bike_share_test <- read.csv("bike_test.csv", header=T)
```


```{r}
str(bike_share_train)
head(bike_share_train)
```


```{r}
# Ignore the casual, registered fields as this sum is equal to count field
bike_share_train <- bike_share_train[,-c(10,11)]
```


```{r}
# Converting integer to factor on training set
bike_share_train$season <- as.factor(bike_share_train$season)
bike_share_train$holiday <- as.factor(bike_share_train$holiday)
bike_share_train$workingday <- as.factor(bike_share_train$workingday)
bike_share_train$weather <- as.factor(bike_share_train$weather)
```


```{r}
# Converting int to factor on test set
bike_share_test$season <- as.factor(bike_share_test$season)
bike_share_test$holiday <- as.factor(bike_share_test$holiday)
bike_share_test$workingday <- as.factor(bike_share_test$workingday)
bike_share_test$weather <- as.factor(bike_share_test$weather)
```
```{r}
bike_share_train
```

```{r}
#Deriving day, hour from datetime field Train & Test
library(lubridate)
bike_share_train$datetime <- ymd_hms(bike_share_train$datetime)
bike_share_train$hour <- lubridate::hour(bike_share_train$datetime)
bike_share_train$day <- lubridate::wday(bike_share_train$datetime)
bike_share_train$month <- lubridate::month(bike_share_train$datetime, label=T)
bike_share_train[,11:13]<-lapply(bike_share_train[,11:13], factor) #converting derived variables into factors (hour, day , month)
head(bike_share_train)
```


```{r}
bike_share_test$datetime <- ymd_hms(bike_share_test$datetime)
bike_share_test$hour <- lubridate::hour(bike_share_test$date)
bike_share_test$day <- lubridate::wday(bike_share_test$date)
bike_share_test$month <- lubridate::month(bike_share_test$date, label=T)
bike_share_test[,10:12]<-lapply(bike_share_test[,10:12], factor) #converting derived variables into factors
```


```{r}
# Removing datetime field 
#bike_share_train$datetime <- NULL
```


```{r}
#Exploratory Data Analysis
library(sqldf)
library(ggplot2)
```


```{r}
# Get the average count of bikes rent by season, hour
season_summary_by_hour <- sqldf('select season, hour, avg(count) as count from bike_share_train group by season, hour')
season_summary_by_hour
```


```{r}
# From this plot it shows, 
# There are more rental in morning(from 7-9th hour) and evening(16-19th hour)
# People rent bikes more in Fall, and much less in Spring
p1<-ggplot(bike_share_train, aes(x=hour, y=count, color=season))+
  geom_point(data = season_summary_by_hour, aes(group = season))+
  geom_line(data = season_summary_by_hour, aes(group = season))+
  ggtitle("Bikes Rent By Season")+ theme_minimal()+
  scale_colour_hue('Season',breaks = levels(bike_share_train$season), 
                   labels=c('spring', 'summer', 'fall', 'winter'))
p1
```


```{r}
# Get the average count of bikes rent by weather, hour
weather_summary_by_hour <- sqldf('select weather, hour, avg(count) as count from bike_share_train group by weather, hour')
weather_summary_by_hour

```


```{r}
# From this plot it shows, 
# People rent bikes more when weather is good
# We see bike rent only at 18th hour when weather is very bad
p2<-ggplot(bike_share_train, aes(x=hour, y=count, color=weather))+
  geom_point(data = weather_summary_by_hour, aes(group = weather))+
  geom_line(data = weather_summary_by_hour, aes(group = weather))+
  ggtitle("Bikes Rent By Weather")+ scale_colour_hue('Weather',breaks = levels(bike_share_train$weather), 
                                                     labels=c('Good', 'Normal', 'Bad', 'Very Bad'))
p2

```


```{r}
# Get the average count of bikes rent by day, hour
day_summary_by_hour <- sqldf('select day, hour, avg(count) as count from bike_share_train group by day, hour')

# From this plot it shows, 
# There are more bikes rent on weekdays during morining and evening
# There are more bikes rent on weekends during daytime
p3<-ggplot(bike_share_train, aes(x=hour, y=count, color=day))+
  geom_point(data = day_summary_by_hour, aes(group = day))+
  geom_line(data = day_summary_by_hour, aes(group = day))+
  ggtitle("Bikes Rent By Weekday")+ scale_colour_hue('Weekday',breaks = levels(bike_share_train$day),
                                                     labels=c('Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'))
p3
```


```{r}
# Splitting the Train dataset
#install.packages("caTools")
library(caTools)
set.seed(123)
split <- sample.split(bike_share_train$count, SplitRatio = 0.75)
training_set <- subset(bike_share_train, split == TRUE)
validation_set <- subset(bike_share_train, split == FALSE)
```
```{r}
training_set
```


```{r}
# Applying Linear Regression model
lmBikeRent <- lm(count~., data = training_set)
summary(lmBikeRent)
```

```{r}
pred <- predict(lmBikeRent, validation_set)
RMSE(validation_set$count,pred) 
```

```{r}
#Residual Plots
# Change the panel layout to 2 x 2
par(mfrow = c(2, 2))
plot(lmBikeRent)
```
Diagnostic Plots:
- Residuals vs Fitted: This plot shows if residuals have non-linear patterns.If you find equally spread residuals around a horizontal line without distinct patterns, that is a good indication you don't have non-linear relationships. 
- Normal Q-Q: This plot shows if residuals are normally distributed.Do residuals follow a straight line well or do they deviate severely? It's good if residuals are lined well on the straight dashed line.
- Scale-Location: It's also called Spread-Location plot. This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). It's good if you see a horizontal line with equally (randomly) spread points.
- Residuals vs Leverage: This plot helps us to find influential cases(outliers) if any. Check if any points fall outside of a dashed line, Cook's distance(meaning they have high Cook's distance score).Those outside points are influential to the regression results.The regression results will be altered if we exclude those cases.
- From plots, it shows there is a pattern at one location as highlighted in plots with green color.

```{r}
#Stepwise Model Selection
# Now performs stepwise model selection by AIC with both directions(Forward, Backward)
#install.packages("car")
library(MASS)
library(car)
```


```{r}
# Performs stepwise model selection by AIC.
lmBikeRentAIC<-stepAIC(lmBikeRent, direction="both")
```


```{r}
# lmBikeRentVIF<-vif(lmBikeRent)
summary(lmBikeRentAIC)
```


```{r}
pred <- predict(lmBikeRentAIC, validation_set)
RMSE(validation_set$count,pred) 
```


```{r}
summary(pred)
```


```{r}
#Log Transformation
# Since we got negative predicted values, let's do log transformation and run regression model again
lmBikeRentLog <- lm(log(count)~., data = training_set)
# Now performs stepwise model selection on log model
lmBikeRentLogAIC <- stepAIC(lmBikeRentLog, direction="both")
lm_predict_validation_log <- predict(lmBikeRentLogAIC,newdata=validation_set)
# As the predicted values are in log format, use exponential(exp) to convert from log to non-log values
lm_predict_validation_nonlog <- exp(lm_predict_validation_log)
RMSE(validation_set$count,lm_predict_validation_nonlog) 
RMSE(log(validation_set$count),lm_predict_validation_log) 

```
```{r}
library(caret) # logistic regression, lm, knn, 
library(glmnet)   # for implementing regularized regression
library(earth)     # for fitting MARS models
library(rpart) # decision tree
library(ipred)       # for fitting bagged decision trees
library(ranger)   # a c++ implementation of random forest 
```


```{r}
# Create blank modelList
cv <- trainControl(
  method = "cv", 
  number = 4, #10
)
modelList <- vector(mode = "list", length = 0)
timeList <- vector(mode = "list", length = 0)

list_model <- c("knn","glmnet","svmRadial","cubist","lm","pcr","pls","bagEarth","ranger","svmLinear") 
# knn,glmnet,svmRadial,lm,pcr,pls: 1 s
# cubist: 3 s
# xgbTree,xgblinear : very slow  # 2,7 min
# ranger: c++ implementation of random forest  # 10 s
# glmnet: lasso and ridge
# bagEarth: Bagged  Multivariate adaptive regression splines # 7s
# pcr: Principal component regression
# pls: Partial least squares
# xgbLinear: eXtreme Gradient Boosting
for (model_name in list_model){
  print(model_name)
  set.seed(7)
  start_time <- lubridate::minute(Sys.time())
  
  Model <- train(count~., data=training_set, method=model_name, metric="RMSE", trControl=cv)
  
  end_time <- lubridate::minute(Sys.time())
  time <- (end_time - start_time)
  print(time)
  
  modelList[[model_name]] <- Model
  timeList[[model_name]] <- time
}
```


```{r}
#bwplot(resamples(modelList),metric="RMSE")
bwplot(resamples(modelList))
bwplot(resamples(modelList),metric="Rsquared")
```


```{r}
time_s <- t(data.frame(timeList)) # transpose
time_df<-data.frame(time_s)
# name id column in R
time_df <- cbind(model_name= rownames(time_df), time= time_df)
rownames(time_df) <- NULL
ggplot(time_df,aes(model_name,time_s))+geom_point()+ geom_line(aes(group = 1))
```
```{r}
modelList["knn"]
```

```{r}
for (model_name in names(modelList) ){
  print(model_name)
  pred <- predict(modelList[[model_name]], validation_set)
  print(RMSE(validation_set$count,pred))
}
```
```{r}
model_gbm <- train(count~., data=training_set, method="gbm", metric="RMSE", trControl=cv)
pred <- predict(model_gbm, validation_set)
print(RMSE(validation_set$count,pred))
```
## ARIMA
```{r}
library(forecast)
library(tidyverse)
```


```{r}
bike_share_train <- read.csv("Bike-Sharing-Dataset_UCI/day.csv", header=T)
bike_share_train
```


```{r}
#class(bike_share_train)
data <- bike_share_train %>% dplyr::select(dteday,cnt)
colnames(data) <- c("date_time","count")
data
```


```{r}
data$date_time <- ymd(data$date_time)
data
```
```{r}
train_test_split_date <- "2012-10-01"

train_tbl <- data %>%
    filter(date_time < ymd(train_test_split_date))

test_tbl <- data %>%
    filter(date_time >= ymd(train_test_split_date))
dim(train_tbl)
dim(test_tbl)
```
```{r}
str(train_tbl)
```

```{r}
library(xts)
xts = xts(train_tbl$count, order.by=train_tbl$date_time)
attr(xts, 'frequency') <- length(xts)/2 # Set the frequency of the xts yearly (in 2 years)
head(xts)
tail(xts)
```

```{r}
ts = as.ts(xts, start = c(2011))
head(ts)
tail(ts)
```

```{r}
#class(train_tbl)
#train_tbl
# Convert train_tbl to time_series
#ts = as.ts(train_tbl,frequency=length(train_tbl)/2) # 2 years
#head(ts)
#tail(ts)

```

```{r}
library(tseries)
tseries::adf.test(xts, alternative = "stationary", k = 0)
```
reject hypothesis --> stationary

```{r}
tscomponents_add <- decompose(ts, type = "additive")
tscomponents_mul <- decompose(ts, type = "multiplicative")
plot(tscomponents_add, col = "red")
```

```{r}
adf.test(xts, alternative = "stationary", k = 0)
```

```{r}
## Differencing a Time Series
xtsdiff1 <- diff(xts, differences=1)
tsdiff1 <- diff(ts, differences=1)
plot.xts(xtsdiff1, col = "blue")
```


```{r}
findfrequency(xts)          # find dominant frequency of original time series
findfrequency(xtsdiff1)     # find dominant frequency of differenced time series
```


```{r}
 ## Selecting a Candidate ARIMA Model
  
print("Selecting a candidate ARIMA Model")
Acf(xtsdiff1, lag.max=60)             # plot a correlogram
Acf(xtsdiff1, lag.max=60, plot=FALSE) # get the autocorrelation values

Pacf(xtsdiff1, lag.max=60)             # plot a partial correlogram
Pacf(xtsdiff1, lag.max=60, plot=FALSE) # get the partial autocorrelation values
```


```{r}
## Fitting an ARIMA Model
tsarima <- auto.arima(head(xts, -120), max.p = 3, max.q = 3, max.d = 3)
```


```{r}
# excluding last 120 time series as test data
print(tsarima)
autoplot(tsarima)
```

```{r}
## Forecasting using an ARIMA Model
tsforecasts <- forecast(tsarima, h = 120) # forecast the next 120 time series
acc <- accuracy(tsforecasts, head(tail(xts, 120), 7))
print(acc)
autoplot(tsforecasts)
```


```{r}
ggplot(data.frame(residuals = tsforecasts$residuals), aes(residuals)) + geom_histogram(bins = 50, aes(y = ..density..), col = "red", fill = "red", alpha = 0.3) + geom_density()# make a histogram
  checkresiduals(tsforecasts)
```

# DNN combine all final
```{r}
library(tidyverse) # metapackage with lots of helpful functions
library(lubridate)
library(caret)
library(recipes)
library(h2o)
```

```{r}
bike =read.csv("bike_train.csv")
bike
```
```{r}
bike <- bike %>% select(-c("casual","registered"))
bike
```


```{r}
## Feature engenearing
bike$datetime <- as_datetime(bike$datetime)
bike$day_week <- wday(bike$datetime, label = TRUE)
bike$month <- lubridate::month(bike$datetime, label = TRUE)
bike$hour <- lubridate::hour(bike$datetime)
```


```{r}
## Converting Factors
bike <- bike %>% 
  mutate(
    holiday = factor(holiday, labels = c('no','yes')),
    workingday = factor(workingday, labels = c('no', 'yes')),
    season = factor(season, labels = c('spring','summer','fall','winter')),
    day_hour = ifelse(hour > 6 & hour < 10, 'between_6_and_10',
                      ifelse(hour > 10 & hour < 17, 'between_10_and_17',
                             ifelse(hour > 17 & hour < 20,
                                    'between_17_and_20', 'between_20_and_6'))),
    day_hour = factor(day_hour, levels = c('between_6_and_10','between_10_and_17','between_17_and_20','between_20_and_6')),
    weather = factor(weather, labels = c('Clear','Cloudy', 'Light_Rain','Heavy_Hain'))
  )
```

Dividing the data into training and test set
```{r}
## 80% of the sample size
nrow(bike)
k=0.8
smp_size <- round( 0.8 * nrow(bike))
print(smp_size)
```
```{r}
train <- bike[1:smp_size, ]
test <- bike[(smp_size+1):nrow(bike), ]
dim(train)
dim(test)
```

```{r}
## -- Recipe --------------------------- ##
blueprint  <- recipe(count ~ ., data = bike) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes(), -hour) %>%  # Remove skewness
  step_center(all_numeric(), - all_outcomes(), -hour) %>% 
  step_scale(all_numeric(), -all_outcomes(), -hour) #%>% 
  #step_ns(hour, deg_free = 6) %>% 
  #step_dummy(all_nominal()) #%>% 
  #step_mutate(count = log ( count + 1))
blueprint 
```
```{r}
prepare <- prep(blueprint, training = train)
prepare
```


```{r}
baked_train <- bake(prepare, new_data = train)
baked_test <- bake(prepare, new_data = test)
head(baked_train)
```

## ML model
```{r}
start_time <- lubridate::minute(Sys.time())
model <- train(count~., data=baked_train, method="gbm", metric="RMSE",verbose=FALSE)
end_time <- lubridate::minute(Sys.time())
time <- (end_time - start_time)
print(time) # 2 min
pred <- predict(model, baked_test)
print(RMSE(baked_test$count,pred)) # 97
```
```{r}
#install.packages("vip")
#library(vip)
#vip::vip(model, num_features = 20, method = "model")
```

```{r}
# Verylong

#start_time <- lubridate::minute(Sys.time())
#model <- train(count~., data=baked_train, method="ranger", metric="RMSE",verbose=FALSE)
#end_time <- lubridate::minute(Sys.time())
#time <- (end_time - start_time)
#print(time)
#pred <- predict(model, baked_test)
#print(RMSE(baked_test$count,pred))
```


```{r}
# VERY long
#start_time <- lubridate::minute(Sys.time())
#model <- train(count~., data=baked_train, method="cubist", metric="RMSE",verbose=FALSE)
#end_time <- lubridate::minute(Sys.time())
#time <- (end_time - start_time)
#print(time)
#pred <- predict(model, baked_test)
#print(RMSE(baked_test$count,pred))
```

```{r}
start_time <- lubridate::minute(Sys.time())
model <- train(count~., data=baked_train, method="glmnet", 
               metric="RMSE",verbose=FALSE)
end_time <- lubridate::minute(Sys.time())
time <- (end_time - start_time)
print(time) # <1 min
pred <- predict(model, baked_test)
print(RMSE(baked_test$count,pred)) # 161
```
```{r}
start_time <- lubridate::minute(Sys.time())
model <- train(count~., data=baked_train, method="ridge", metric="RMSE",verbose=FALSE)
end_time <- lubridate::minute(Sys.time())
time <- (end_time - start_time)
print(time) # < 1 min
pred <- predict(model, baked_test)
print(RMSE(baked_test$count,pred)) # 161
```


```{r}
#install.packages("parsnip")
library(parsnip)
glmnet_fit <- parsnip::linear_reg(mode = "regression", penalty = 0.01, mixture = 0.5) %>%
    set_engine("glmnet") %>%
    fit.model_spec(count ~ ., data = baked_train) # < 10 s
pred <- glmnet_fit %>% predict(new_data = baked_test) 
print(RMSE(baked_test$count,pred[[".pred"]])) # 161
```
```{r}
install.packages("xgboost")
```

```{r}
model <- parsnip::boost_tree(mode = "regression") %>%
    set_engine("xgboost") %>%
    fit.model_spec(count ~ ., data = baked_train) # < 10 s
pred <- model %>% predict(new_data = baked_test) 
print(RMSE(baked_test$count,pred[[".pred"]])) # 67

```
```{r}
#install.packages("vip")
vip::vip(model, num_features = 20, method = "model")
```

```{r}
#install.packages("ranger")
model <- parsnip::rand_forest(mode = "regression") %>%
    set_engine("ranger") %>%
    fit.model_spec(count ~ ., data = baked_train) # < 10 s
pred <- model %>% predict(new_data = baked_test) 
print(RMSE(baked_test$count,pred[[".pred"]])) #107
```


```{r}
model <- parsnip::mars(mode = "regression") %>%
    set_engine("earth") %>%
    fit.model_spec(count ~ ., data = baked_train) # < 10 s
pred <- model %>% predict(new_data = baked_test) 
print(RMSE(baked_test$count,pred[[".pred"]])) # 133
```
```{r}
model <- parsnip::linear_reg(mode = "regression",penalty = 0.01, mixture = 0.5) %>%
    set_engine("glmnet") %>%
    fit.model_spec(count ~ ., data = baked_train) # < 10 s
pred <- model %>% predict(new_data = baked_test) 
print(RMSE(baked_test$count,pred[[".pred"]])) # 161
```
```{r}
#install.packages("liquidSVM")
model <- parsnip::svm_rbf(mode = "regression") %>%
    set_engine("liquidSVM") %>%
    fit.model_spec(count ~ ., data = baked_train) # < 20 s
pred <- model %>% predict(new_data = baked_test) 
print(RMSE(baked_test$count,pred[[".pred"]])) # 230
```



```{r}
library(h2o)
h2o.init()
```
Data for H2O:
- No date time POSIXct
- No order column --> must be converted to factor
```{r}
baked_train
```

```{r}
# Convert order to factor
baked_train$day_week <- factor(baked_train$day_week,ordered=FALSE)
baked_train$month <- factor(baked_train$month,ordered=FALSE)
baked_test$day_week <- factor(baked_test$day_week,ordered=FALSE)
baked_test$month <- factor(baked_test$month,ordered=FALSE)
baked_train
```


```{r}
# convert training data to h2o object
train_h2o <- as.h2o(baked_train %>% select(-datetime))
test_h2o <- as.h2o(baked_test %>% select(-datetime))
# set the response column to Sale_Price
response <- "count"
n_features <- length(setdiff(names(baked_train %>% select(-datetime)), "count"))
# set the predictor names
predictors <- setdiff(colnames(baked_train %>% select(-datetime)), response)
```

GBM H2O
```{r}
start_time <- lubridate::minute(Sys.time())
h2o_model <- h2o.gbm(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    seed = 123
)
end_time <- lubridate::minute(Sys.time())
time <- (end_time - start_time)
print(time) # <10s
#h2o_model
pred <-h2o.predict(h2o_model, newdata = test_h2o)
print(RMSE(test_h2o$count,pred) ) # 132
```
```{r}
h2o_model
```


```{r}
vip::vip(h2o_model, num_features = 25, bar = TRUE)
```

GLM H2O
```{r}
start_time <- lubridate::minute(Sys.time())
h2o_model <- h2o.glm(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    seed = 123
)
end_time <- lubridate::minute(Sys.time())
time <- (end_time - start_time)
print(time) # <10s
#h2o_model
pred <-h2o.predict(h2o_model, newdata = test_h2o)
print(RMSE(test_h2o$count,pred) ) # 187
```


```{r}
start_time <- lubridate::minute(Sys.time())
h2o_model <- h2o.randomForest(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    seed = 123
)
end_time <- lubridate::minute(Sys.time())
time <- (end_time - start_time)
print(time) # <10s
#h2o_model
pred <-h2o.predict(h2o_model, newdata = test_h2o)
print(RMSE(test_h2o$count,pred) ) # 135
```
```{r}
vip::vip(h2o_model, num_features = 25, bar = TRUE)
```

Deep learning H2O
```{r}
start_time <- lubridate::minute(Sys.time())
h2o_dl <- h2o.deeplearning(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    hidden = 25,
    seed = 123
)
end_time <- lubridate::minute(Sys.time())
time <- (end_time - start_time)
print(time) # <0
pred <-h2o.predict(h2o_dl, newdata = test_h2o)
print(RMSE(test_h2o$count,pred) ) # 183

```


```{r}
start_time <- lubridate::minute(Sys.time())
h2o_dl <- h2o.deeplearning(
    x = predictors, 
    y = response,
    training_frame = train_h2o, 
    hidden = c(25,10),
    rate=0.01, 
    l1=1e-5,                        ## add some L1/L2 regularization
    l2=1e-5,
    seed = 123
)
end_time <- lubridate::minute(Sys.time())
time <- (end_time - start_time)
print(time) # <0
pred <-h2o.predict(h2o_dl, newdata = test_h2o)
print(RMSE(test_h2o$count,pred) ) # 186
```

## Time series

```{r}
library(tidyverse) # metapackage with lots of helpful functions
library(lubridate)
library(caret)
library(recipes)
```


```{r}
bike =read.csv("bike_train.csv")
## Feature engenearing
bike$datetime <- as_datetime(bike$datetime)
bike$day_week <- wday(bike$datetime, label = TRUE)
bike$month <- lubridate::month(bike$datetime, label = TRUE)
bike$hour <- lubridate::hour(bike$datetime)
## Converting Factors
bike <- bike %>% 
  mutate(
    holiday = factor(holiday, labels = c('no','yes')),
    workingday = factor(workingday, labels = c('no', 'yes')),
    season = factor(season, labels = c('spring','summer','fall','winter')),
    day_hour = ifelse(hour > 6 & hour < 10, 'between_6_and_10',
                      ifelse(hour > 10 & hour < 17, 'between_10_and_17',
                             ifelse(hour > 17 & hour < 20,
                                    'between_17_and_20', 'between_20_and_6'))),
    day_hour = factor(day_hour, levels = c('between_6_and_10','between_10_and_17','between_17_and_20','between_20_and_6')),
    weather = factor(weather, labels = c('Clear','Cloudy', 'Light_Rain','Heavy_Hain'))
  )
## 80% of the sample size
nrow(bike)
k=0.8
smp_size <- round( 0.8 * nrow(bike))
print(smp_size)
train <- bike[1:smp_size, ]
test <- bike[(smp_size+1):nrow(bike), ]
dim(train)
dim(test)

## -- Recipe --------------------------- ##
blueprint  <- recipe(count ~ ., data = bike) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes(), -hour) %>%  # Remove skewness
  step_center(all_numeric(), - all_outcomes(), -hour) %>% 
  step_scale(all_numeric(), -all_outcomes(), -hour) #%>% 
  #step_ns(hour, deg_free = 6) %>% 
  #step_dummy(all_nominal()) #%>% 
  #step_mutate(count = log ( count + 1))
prepare <- prep(blueprint, training = train)
baked_train <- bake(prepare, new_data = train)
baked_test <- bake(prepare, new_data = test)
head(baked_train)
```
```{r}
library(xts)
train_xts = xts(baked_train$count, order.by=baked_train$datetime)
test_xts =  xts(baked_test$count, order.by=baked_test$datetime)
attr(xts, 'frequency') <- length(xts)/2 # Set the frequency of the xts yearly (in 2 years)
head(train_xts)
tail(train_xts)
```


```{r}
tseries::adf.test(train_xts, alternative = "stationary", k = 0)
```


```{r}
## Decomposing Time Series
ts = as.ts(xts, start = c(2011))
tscomponents <- decompose(ts)
plot(tscomponents, col = "red")
```


```{r}
## Differencing a Time Series
xtsdiff1 <- diff(xts, differences=1)
plot.xts(xtsdiff1, col = "blue")
```
```{r}
library(forecast)
```


```{r}
## Selecting a Candidate ARIMA Model

print("Selecting a candidate ARIMA Model")
Acf(xtsdiff1, lag.max=60)             # plot a correlogram
Acf(xtsdiff1, lag.max=60, plot=FALSE) # get the autocorrelation values

Pacf(xtsdiff1, lag.max=60)             # plot a partial correlogram
Pacf(xtsdiff1, lag.max=60, plot=FALSE) # get the partial autocorrelation values
```


```{r}
## Fitting an ARIMA Model
tsarima <- auto.arima(train_xts, max.p = 3, max.q = 3, max.d = 3)
print(tsarima)
```


```{r}
autoplot(tsarima)
```
```{r}
dim(baked_test)
```


```{r}
tsforecasts <- forecast(tsarima, h = 2177)
tsforecasts
```

```{r}
#baked_test$count
print(RMSE(baked_test$count,tsforecasts$mean)) # 317
```

```{r}
autoplot(tsforecasts)
```


```{r}
plot.ts(tsforecasts$residuals)  
```

```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```



```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```

